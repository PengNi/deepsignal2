{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--conv-in', type=int, default=4,\n",
    "                        help='Input sequence features')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(tensor, dim=-1):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / torch.sqrt(squared_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squash(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, eps=10e-21, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, s):\n",
    "        n = nn.norm(s,axis=-1,keepdims=True)\n",
    "        return (1 - 1/(nn.math.exp(n)+self.eps))*(s/(n+self.eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_routing(x, iterations=3):\n",
    "\n",
    "    N = 32*6*6 # previous layer\n",
    "    N1 = 10 # next layer\n",
    "    B = x.shape[0]\n",
    "\n",
    "    b = torch.zeros(B,N1,N,1, 1).to(x.device)\n",
    "    for _ in range(iterations):        \n",
    "        c = F.softmax(b, dim=1)  \n",
    "        s = torch.sum(x.matmul(c), dim=2).squeeze(-1)\n",
    "        v = squash(s)\n",
    "\n",
    "        b = b + v[:,:,None,None,:].matmul(x)\n",
    "\n",
    "\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCapsuleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Create a primary capsule layer with the methodology described in 'Efficient-CapsNet: Capsule Network with Self-Attention Routing'. \n",
    "    Properties of each capsule s_n are exatracted using a 2D depthwise convolution.\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    kernel_size[h,w]: int\n",
    "        depthwise conv kernel dimension\n",
    "    conv_num: int\n",
    "        number of primary capsules\n",
    "    feature_dimension: int\n",
    "        primary capsules dimension (number of properties)\n",
    "    conv_stride: int\n",
    "        depthwise conv strides\n",
    "    Methods\n",
    "    -------\n",
    "    call(inputs)\n",
    "        compute the primary capsule layer\n",
    "    \"\"\"\n",
    "    def __init__(self,conv_in, conv_out, kernel_size,conv_stride,feature_dimension,conv_num=1):\n",
    "        super().__init__()\n",
    "        self.feature_dimension=feature_dimension\n",
    "        self.conv_num=conv_num\n",
    "        self.primary_capsule_layer = \\\n",
    "            nn.ModuleList([nn.Conv2d(conv_in, conv_out, kernel_size, conv_stride) for _ in range(conv_num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        capsules = [conv(x) for conv in self.primary_capsule_layer]  \n",
    "        capsules_reshaped = [c.reshape(self.conv_num,self.feature_dimension) for c in capsules]  \n",
    "        return Squash()(capsules_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsLayer(nn.Module):\n",
    "    def __init__(self,nclasses, out_channels_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(1e-3 * torch.randn(1,nclasses,32*6*6,out_channels_dim,8))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x[:,None,...,None]\n",
    "        u_hat = self.W.matmul(x)\n",
    "        class_capsules = dynamic_routing(u_hat)\n",
    "        return class_capsules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv1d(1,256,9)\n",
    "        self.primary_layer = PrimaryCapsuleLayer()\n",
    "        self.caps_layer = CapsLayer(nclasses=10, out_channels_dim=16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)  \n",
    "        x = self.primary_layer(x)  \n",
    "        x = self.caps_layer(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsignal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
