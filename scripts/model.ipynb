{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--conv-in\", type=int, default=4, help=\"Input sequence features\"\n",
    "    )\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(tensor, dim=-1):\n",
    "    squared_norm = (tensor**2).sum(dim=dim, keepdim=True)\n",
    "    scale = squared_norm / (1 + squared_norm)\n",
    "    return scale * tensor / torch.sqrt(squared_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squash(nn.Module):\n",
    "    def __init__(self, eps=10e-21, **kwargs):\n",
    "        super(Squash,self).__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, s):\n",
    "        n = torch.norm(s, dim=-1, keepdim=True)\n",
    "        return (1 - 1 / (torch.exp(n) + self.eps)) * (s / (n + self.eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_routing(x, iterations=3):\n",
    "    x=x.unsqueeze(-1)\n",
    "    N = x.shape[1]  # previous layer\n",
    "    N1 = 1 # next layer\n",
    "    B = x.shape[0]\n",
    "    #feature_dim = x.shape[2]\n",
    "\n",
    "    b = torch.zeros(B, N1, N,1,1).to(x.device)\n",
    "    for _ in range(iterations):\n",
    "        print('x shape: {}'.format(x.shape))\n",
    "        c = F.softmax(b, dim=1)\n",
    "        print('c shape: {}'.format(c.shape))\n",
    "        a = x.matmul(c)\n",
    "        #print('a shape: {}'.format(a.shape))\n",
    "        s = torch.sum(a, dim=2).squeeze(-1)\n",
    "        #print('s shape: {}'.format(s.shape))\n",
    "        v = squash(s)\n",
    "        #print('v shape: {}'.format(v.shape))\n",
    "        #print('x shape: {}'.format(x.shape))\n",
    "        y=v.matmul(x)\n",
    "        #print('y shape: {}'.format(y.shape))\n",
    "        #print('b shape: {}'.format(b.shape))\n",
    "        b = b + y\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 1, 1])\n",
      "torch.Size([1, 1, 5, 105, 1])\n",
      "torch.Size([1, 1, 105])\n"
     ]
    }
   ],
   "source": [
    "c=F.softmax(torch.zeros(1, 1, 5,1,1), dim=1)\n",
    "#print(c)\n",
    "print(c.shape)\n",
    "x = torch.randn(1, 5, 21 * 5,1)\n",
    "a = x.matmul(c)\n",
    "#print(a)\n",
    "print(a.shape)\n",
    "s = torch.sum(x.matmul(c), dim=2).squeeze(-1)\n",
    "#print(s)\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 105])\n"
     ]
    }
   ],
   "source": [
    "class PrimaryCapsuleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Create a primary capsule layer with the methodology described in 'Efficient-CapsNet: Capsule Network with Self-Attention Routing'.\n",
    "    Properties of each capsule s_n are exatracted using a 1D depthwise convolution.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    kernel_size[w]: int\n",
    "        depthwise conv kernel dimension\n",
    "    conv_num: int\n",
    "        number of primary capsules\n",
    "    feature_dimension: int\n",
    "        primary capsules dimension (number of properties)\n",
    "    conv_stride: int\n",
    "        depthwise conv strides\n",
    "    Methods\n",
    "    -------\n",
    "    forward(inputs)\n",
    "        compute the primary capsule layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, conv_in=2, feature_dimension=21*5, kernel_size=2, conv_num=5,base_num=21\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv_out = feature_dimension//(conv_num*base_num)\n",
    "        self.conv_num = conv_num\n",
    "        self.primary_capsule_layer = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(conv_in, self.conv_out, kernel_size,dilation=conv_stride, padding='same')\n",
    "                for conv_stride in range(1,conv_num+1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        capsules = [conv(x) for conv in self.primary_capsule_layer]\n",
    "        #capsules_reshaped = [\n",
    "        #    c.reshape(self.conv_num, self.feature_dimension) for c in capsules\n",
    "        #]\n",
    "        output_tensor = torch.cat(capsules, dim=1)\n",
    "        return Squash()(output_tensor)\n",
    "def test_for_primary_capsule_layer():\n",
    "    input = torch.rand(1,2,105)\n",
    "    layer = PrimaryCapsuleLayer()\n",
    "    print(layer(input).shape)\n",
    "test_for_primary_capsule_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 105])\n",
      "torch.Size([1, 1, 105])\n",
      "torch.Size([1, 1, 105])\n",
      "torch.Size([1, 1, 105])\n",
      "torch.Size([1, 1, 105])\n",
      "torch.Size([1, 5, 105])\n",
      "torch.Size([1, 1, 105])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the input and output channels\n",
    "in_channels = 2\n",
    "out_channels = 1\n",
    "\n",
    "# Define the kernel size and dilation\n",
    "kernel_size = 2\n",
    "\n",
    "# Define the 1D dilated convolution layers\n",
    "conv1d_list = nn.ModuleList()\n",
    "for dilation in range(1, 6):\n",
    "    padding = 'same'\n",
    "    conv1d_list.append(nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation,padding = padding))\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = torch.randn(1, in_channels, 21 * 5)\n",
    "\n",
    "# Apply the 1D dilated convolutions to the input tensor\n",
    "output_tensor_list = []\n",
    "for conv1d in conv1d_list:\n",
    "    print(conv1d(input_tensor).shape)\n",
    "    output_tensor_list.append(conv1d(input_tensor))\n",
    "\n",
    "# Concatenate the output tensors along the channel dimension\n",
    "output_tensor = torch.cat(output_tensor_list, dim=1)\n",
    "\n",
    "print(output_tensor.shape)\n",
    "print(dynamic_routing(output_tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsLayer(nn.Module):\n",
    "    def __init__(self, num_capsules=1, num_route_nodes=5, in_channels=105, out_channels=20):\n",
    "        super(CapsLayer,self).__init__()\n",
    "        self.W = nn.Parameter(0.01 * \n",
    "            torch.randn(1,num_capsules, num_route_nodes, out_channels, in_channels)\n",
    "        )\n",
    "        print('W shape: {}'.format(self.W.shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:,None,...,None]#x.unsqueeze(1).unsqueeze(4)\n",
    "        #x = x.unsqueeze(-1)\n",
    "        #print('W expand shape: {}'.format(self.W[:, None, :, :, :].shape))\n",
    "        #print('CapsLayer input shape: {}'.format(x.shape))\n",
    "        #print('CapsLayer input expand shape: {}'.format(x[ :, :, None, :].shape))\n",
    "        # (batch_size, num_caps, num_route_nodes, out_channels, 1)\n",
    "        print('x shape: {}'.format(x.shape))\n",
    "        u_hat = torch.matmul(self.W, x)#(x @ self.W).squeeze(2)\n",
    "        #u=u_hat.squeeze(-1)\n",
    "        u_hat = u_hat.squeeze(-1)\n",
    "        print('u_hat shape: {}'.format(u_hat.shape))\n",
    "        class_capsules = dynamic_routing(u_hat)\n",
    "        return class_capsules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1, 10, 10, 20, 1)\n",
    "b = torch.rand(1, 1, 10, 1, 1)\n",
    "c = a.matmul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape: torch.Size([1, 1, 5, 20, 105])\n",
      "x shape: torch.Size([1, 1, 5, 105, 1])\n",
      "u_hat shape: torch.Size([1, 1, 5, 20])\n",
      "x shape: torch.Size([1, 1, 5, 20, 1])\n",
      "c shape: torch.Size([1, 1, 1, 1, 1])\n",
      "x shape: torch.Size([1, 1, 5, 20, 1])\n",
      "c shape: torch.Size([1, 1, 5, 1, 1])\n",
      "x shape: torch.Size([1, 1, 5, 20, 1])\n",
      "c shape: torch.Size([1, 1, 5, 1, 1])\n",
      "torch.Size([1, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(1,5,105)\n",
    "layer = CapsLayer()\n",
    "print(layer(input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape: torch.Size([1, 1, 5, 20, 105])\n",
      "x shape: torch.Size([1, 1, 5, 105, 1])\n",
      "u_hat shape: torch.Size([1, 1, 5, 20])\n",
      "x shape: torch.Size([1, 1, 5, 20, 1])\n",
      "c shape: torch.Size([1, 1, 1, 1, 1])\n",
      "x shape: torch.Size([1, 1, 5, 20, 1])\n",
      "c shape: torch.Size([1, 1, 5, 1, 1])\n",
      "x shape: torch.Size([1, 1, 5, 20, 1])\n",
      "c shape: torch.Size([1, 1, 5, 1, 1])\n",
      "torch.Size([1, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet,self).__init__()\n",
    "        self.primary_layer = PrimaryCapsuleLayer()\n",
    "        self.caps_layer = CapsLayer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.primary_layer(x)\n",
    "        x = self.caps_layer(x)\n",
    "        return x\n",
    "def test_for_caps_net():\n",
    "    input = torch.rand(1,2,105)\n",
    "    model = CapsNet()\n",
    "    print(model(input).shape)\n",
    "test_for_caps_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsignal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
