{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pod5 as p5\n",
    "import pysam\n",
    "from multiprocessing import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signal_from_pod5(pod5_path):\n",
    "    signals={}\n",
    "    with p5.Reader(pod5_path) as reader:\n",
    "        for read_record in reader.reads():\n",
    "            signals[str(read_record.read_id)] = {'signal':read_record.signal,'shift':read_record.calibration.offset,'scale':read_record.calibration.scale}#不加str会变成UUID，很奇怪\n",
    "    return signals\n",
    "\n",
    "def extract_move_from_bam(bam_path):\n",
    "    read_dict=dict()\n",
    "    bamfile = pysam.AlignmentFile(bam_path, \"rb\",check_sq=False)\n",
    "    try:\n",
    "        for read in bamfile.fetch():\n",
    "            \n",
    "            tags=dict(read.tags)\n",
    "            mv_tag=tags['mv']\n",
    "            ts_tag=tags['ts']\n",
    "            sm_tag=tags[\"sm\"]\n",
    "            sd_tag=tags[\"sd\"]\n",
    "            read_dict.update({read.query_name:{\"sequence\":read.query_sequence,\"stride\":mv_tag[0],\"mv_table\":np.array(mv_tag[1:]),\"num_trimmed\":ts_tag,\"shift\":sm_tag,\"scale\":sd_tag}})\n",
    "    except ValueError:\n",
    "        print('bam don\\'t has index')\n",
    "        for read in bamfile.fetch(until_eof=True,multiple_iterators=True):\n",
    "            tags=dict(read.tags)\n",
    "            mv_tag=tags['mv']\n",
    "            ts_tag=tags['ts']\n",
    "            sm_tag=tags[\"sm\"]\n",
    "            sd_tag=tags[\"sd\"]\n",
    "            read_dict[read.query_name] = {\"sequence\":read.query_sequence,\"stride\":mv_tag[0],\"mv_table\":np.array(mv_tag[1:]),\"num_trimmed\":ts_tag,\"shift\":sm_tag,\"scale\":sd_tag}\n",
    "    return read_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bam_path = '/homeb/xiaoyf/data/HG002/example/bam/sorted_has_moves.bam'\n",
    "bamfile = pysam.AlignmentFile(bam_path, \"rb\",check_sq=False)\n",
    "bamfile.has_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for read in bamfile.get_index_statistics():\n",
    "    #print(read)\n",
    "    print(type(read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_and_move_table=extract_move_from_bam(bam_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_and_move_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base2code_dna = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4,\n",
    "                 'W': 4, 'S': 4, 'M': 4, 'K': 4, 'R': 4,\n",
    "                 'Y': 4, 'B': 4, 'V': 4, 'D': 4, 'H': 4,\n",
    "                 'Z': 4}  # set 4 for all bases except ACGT, for now\n",
    "def norm_signal_read_id(signal):\n",
    "    shift_scale_norm={}\n",
    "    signal_norm={}\n",
    "    shift_scale_norm={}\n",
    "    shift_scale_norm['shift']=(signal['to_norm_shift']/signal['to_pA_scale'])-signal['to_pA_shift']\n",
    "    shift_scale_norm['scale']=(signal['to_norm_scale']/signal['to_pA_scale'])\n",
    "    num_trimmed=signal[\"num_trimmed\"]\n",
    "    signal_norm=(signal['signal'][num_trimmed:] - shift_scale_norm['shift']) / shift_scale_norm['scale']        \n",
    "    return signal_norm\n",
    "\n",
    "def read_from_pod5_bam(pod5_path,bam_path,read_id=None):\n",
    "    read={}\n",
    "    signal = extract_signal_from_pod5(pod5_path)\n",
    "    seq_move = extract_move_from_bam(bam_path)\n",
    "    if read_id is not None:\n",
    "        if seq_move[read_id]['sequence'] is not None:\n",
    "            if signal[read_id] is not None:\n",
    "                read[read_id]={\"sequence\":seq_move[read_id]['sequence'],\"signal\":signal[read_id]['signal'],'mv_table':seq_move[read_id]['mv_table'],\n",
    "                               \"num_trimmed\":seq_move[read_id]['num_trimmed'],\"to_norm_shift\":seq_move[read_id]['shift'],\n",
    "                               \"to_norm_scale\":seq_move[read_id]['scale'],\"stride\":seq_move[read_id]['stride'],\n",
    "                               'to_pA_shift':signal[read_id]['shift'],\n",
    "                               'to_pA_scale':signal[read_id][\"scale\"]}\n",
    "    else:\n",
    "        for read_id in seq_move.keys():\n",
    "            if seq_move[read_id]['sequence'] is not None:\n",
    "                if signal[read_id] is not None:\n",
    "                    read[read_id]={\"sequence\":seq_move[read_id]['sequence'],\"signal\":signal[read_id]['signal'],'mv_table':seq_move[read_id]['mv_table'],\n",
    "                               \"num_trimmed\":seq_move[read_id]['num_trimmed'],\"to_norm_shift\":seq_move[read_id]['shift'],\n",
    "                               \"to_norm_scale\":seq_move[read_id]['scale'],\"stride\":seq_move[read_id]['stride'],\n",
    "                               'to_pA_shift':signal[read_id]['shift'],\n",
    "                               'to_pA_scale':signal[read_id][\"scale\"]}\n",
    "    return read\n",
    "\n",
    "def caculate_feature_for_each_base(read,base_num=0):   \n",
    "    feature={}\n",
    "    for read_id in read.keys():\n",
    "        feature[read_id]=[]\n",
    "        sequence = read[read_id]['sequence']\n",
    "        movetable = read[read_id]['mv_table']\n",
    "        stride = read[read_id]['stride']\n",
    "        #num_trimmed = read[read_id]['num_trimmed']\n",
    "        trimed_signals = norm_signal_read_id(read[read_id])#筛掉背景信号,norm\n",
    "        move_pos = np.append(np.argwhere(movetable == 1).flatten(), len(movetable))\n",
    "        #print(len(move_pos))\n",
    "        for move_idx in range(len(move_pos) - 1):\n",
    "            start, end = move_pos[move_idx], move_pos[move_idx + 1]\n",
    "            signal=trimed_signals[(start * stride):(end * stride)].tolist()\n",
    "            mean=np.mean(signal)\n",
    "            std=np.std(signal)\n",
    "            num=end-start        \n",
    "            feature[read_id].append({'signal':signal,'std':std,'mean':mean,'num':num*read[read_id][stride],'base':sequence[move_idx]})\n",
    "    if base_num==0:\n",
    "        return feature\n",
    "    else:\n",
    "        windows_size=base_num/2\n",
    "        for read_id in feature.keys():\n",
    "            for i in range(len(feature[read_id])):\n",
    "                nbase=[]\n",
    "                nsig=[]\n",
    "                if i<windows_size:                   \n",
    "                    if i!=0:\n",
    "                        for k in range(i):\n",
    "                            nbase=nbase+feature[read_id][k]['base']*feature[read_id][k][num]\n",
    "                            nsig=nsig+feature[read_id][k]['signal']\n",
    "                    nbase=nbase+feature[read_id][i]['base']*(windows_size-i)*feature[read_id][k][num]\n",
    "                    nbase=nbase+feature[read_id][i]['base']*feature[read_id][k][num]\n",
    "                    for k in range(i,i+windows_size):\n",
    "                        nbase=nbase+feature[read_id][k]['base']*feature[read_id][k][num]\n",
    "                    nsig=nsig+feature[read_id][i]['signal']*(windows_size-i)\n",
    "                    nsig=nsig+feature[read_id][i]['signal']\n",
    "                    for k in range(i,i+windows_size):\n",
    "                        nsig=nsig+feature[read_id][k]['signal']\n",
    "                elif (len(feature[read_id])-1)-i<windows_size:\n",
    "                    for k in range(i-windows_size,i):\n",
    "                        nbase=nbase+feature[read_id][k]['base']*feature[read_id][k][num]\n",
    "                    nbase=nbase+feature[read_id][i]['base']*feature[read_id][k][num]\n",
    "                    for k in range(i-windows_size,i):\n",
    "                        nsig=nsig+feature[read_id][k]['signal']\n",
    "                    nsig=nsig+feature[read_id][i]['signal']                   \n",
    "                    if i!=len(feature[read_id])-1:\n",
    "                        for k in range(i,len(feature[read_id])-1):\n",
    "                            nbase=nbase+feature[read_id][k]['base']*feature[read_id][k][num]\n",
    "                            nsig=nsig+feature[read_id][k]['signal']\n",
    "                    nbase=nbase+feature[read_id][i]['base']*(windows_size-((len(feature[read_id])-1)-i))*feature[read_id][k][num]\n",
    "                    nsig=nsig+feature[read_id][i]['signal']*(windows_size-((len(feature[read_id])-1)-i))\n",
    "                else:\n",
    "                    for k in range(i-windows_size,i):\n",
    "                        nbase=nbase+feature[read_id][k]['base']*feature[read_id][k][num]\n",
    "                    nbase=nbase+feature[read_id][i]['base']*feature[read_id][k][num]\n",
    "                    for k in range(i,i+windows_size):\n",
    "                        nbase=nbase+feature[read_id][k]['base']*feature[read_id][k][num]\n",
    "                    for k in range(i-windows_size,i):\n",
    "                        nsig=nsig+feature[read_id][k]['signal']\n",
    "                    nsig=nsig+feature[read_id][i]['signal']\n",
    "                    for k in range(i,i+windows_size):\n",
    "                        nsig=nsig+feature[read_id][k]['signal']\n",
    "                feature[read_id][i].update({'nbase':nbase,'nsig':nsig})\n",
    "        return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E::idx_find_and_load] Could not retrieve index file for '/homeb/xiaoyf/data/HG002/example/bam/has_moves.bam'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bam don't has index\n"
     ]
    }
   ],
   "source": [
    "pod5_path = '/homeb/xiaoyf/data/HG002/example/pod5/output.pod5'\n",
    "bam_path = '/homeb/xiaoyf/data/HG002/example/bam/has_moves.bam'\n",
    "read=read_from_pod5_bam(pod5_path,bam_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=caculate_feature_for_each_base(read,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['signal', 'std', 'mean', 'num'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature['0000b1ad-fdaf-49e6-bc11-cbe93270e3a3'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_feature(feature):\n",
    "    with open('/homeb/xiaoyf/data/HG002/example/feature.txt','w') as f:\n",
    "        for read_id in feature.keys():\n",
    "            #f.write(read_id+'\\t')\n",
    "            for i in range(len(feature[read_id])):\n",
    "                f.write(str(feature[read_id][i]['nbase'])+'\\t'+str(feature[read_id][i]['nsig'])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nbase'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m write_feature(feature)\n",
      "Cell \u001b[0;32mIn[40], line 6\u001b[0m, in \u001b[0;36mwrite_feature\u001b[0;34m(feature)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m read_id \u001b[39min\u001b[39;00m feature\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m      4\u001b[0m     \u001b[39m#f.write(read_id+'\\t')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(feature[read_id])):\n\u001b[0;32m----> 6\u001b[0m         f\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(feature[read_id][i][\u001b[39m'\u001b[39;49m\u001b[39mnbase\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(feature[read_id][i][\u001b[39m'\u001b[39m\u001b[39mnsig\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nbase'"
     ]
    }
   ],
   "source": [
    "write_feature(feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsignal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
