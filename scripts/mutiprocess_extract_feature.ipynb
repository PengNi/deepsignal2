{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pod5 as p5\n",
    "import pysam\n",
    "from multiprocessing import Queue  # ,Manager,Pool\n",
    "import time\n",
    "import os\n",
    "import signal\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "\n",
    "def handler(signum, frame):\n",
    "    print(\"hello world!\")\n",
    "\n",
    "\n",
    "signal.signal(signal.SIGPIPE, signal.SIG_IGN)  # 忽略SIGPIPE信号\n",
    "logger = logging.getLogger(\"test_logger\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "test_log = logging.FileHandler(\n",
    "    \"/home/xiaoyf/methylation/deepsignal/log/process.log\", \"a\", encoding=\"utf-8\"\n",
    ")\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s - %(filename)s - line:%(lineno)d - %(levelname)s - %(message)s -%(process)s\"\n",
    ")\n",
    "test_log.setFormatter(formatter)\n",
    "# 加载文件到logger对象中\n",
    "logger.addHandler(test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signal_from_pod5(pod5_path) -> np.array:\n",
    "    signals = []\n",
    "    with p5.Reader(pod5_path) as reader:\n",
    "        for read_record in reader.reads():\n",
    "            if read_record.signal is None:\n",
    "                logger.critical(\n",
    "                    \"Signal is None for read id {}\".format(read_record.read_id)\n",
    "                )\n",
    "            # signals[str(read_record.read_id)] = {'signal':read_record.signal,'shift':read_record.calibration.offset,'scale':read_record.calibration.scale}#不加str会变成UUID，很奇怪\n",
    "            signals.append(\n",
    "                [\n",
    "                    str(read_record.read_id),\n",
    "                    read_record.signal.astype(np.int8),\n",
    "                    np.int8(read_record.calibration.offset),\n",
    "                    np.float16(read_record.calibration.scale),\n",
    "                ]\n",
    "            )\n",
    "            # 0:read_id,1:signal,2:shift,3:scale\n",
    "    return np.array(signals, dtype=object)  # np.array is small than list\n",
    "\n",
    "\n",
    "def extract_move_from_bam(bam_path) -> np.array:\n",
    "    seq_move = []\n",
    "    bamfile = pysam.AlignmentFile(bam_path, \"rb\", check_sq=False)\n",
    "    try:\n",
    "        for read in bamfile.fetch(until_eof=True):  # 暂时不使用索引，使用返回是空值\n",
    "            # print(read.query_name)\n",
    "            tags = dict(read.tags)\n",
    "            mv_tag = tags[\"mv\"]\n",
    "            ts_tag = tags[\"ts\"]\n",
    "            sm_tag = tags[\"sm\"]\n",
    "            sd_tag = tags[\"sd\"]\n",
    "            # read.update({read.query_name:{\"sequence\":read.query_sequence,\"stride\":mv_tag[0],\"mv_table\":np.array(mv_tag[1:]),\"num_trimmed\":ts_tag,\"shift\":sm_tag,\"scale\":sd_tag}})\n",
    "            seq_move.append(\n",
    "                [\n",
    "                    read.query_name,\n",
    "                    read.query_sequence,\n",
    "                    np.int8(mv_tag[0]),\n",
    "                    np.array(mv_tag[1:], dtype=np.int8),\n",
    "                    np.int8(ts_tag),\n",
    "                    np.float16(sm_tag),\n",
    "                    np.float16(sd_tag),\n",
    "                ]\n",
    "            )\n",
    "    except ValueError:\n",
    "        print(\"bam don't has index\")\n",
    "        for read in bamfile.fetch(until_eof=True, multiple_iterators=False):\n",
    "            tags = dict(read.tags)\n",
    "            mv_tag = tags[\"mv\"]\n",
    "            ts_tag = tags[\"ts\"]\n",
    "            sm_tag = tags[\"sm\"]\n",
    "            sd_tag = tags[\"sd\"]\n",
    "            seq_move.append(\n",
    "                [\n",
    "                    read.query_name,\n",
    "                    read.query_sequence,\n",
    "                    np.int8(mv_tag[0]),\n",
    "                    np.array(mv_tag[1:], dtype=np.int8),\n",
    "                    np.int8(ts_tag),\n",
    "                    np.float16(sm_tag),\n",
    "                    np.float16(sd_tag),\n",
    "                ]\n",
    "            )\n",
    "            # 0:read_id,1:sequence,2:stride,3:mv_table,4:num_trimmed,5:to_norm_shift,6:to_norm_scale\n",
    "            # read[read.query_name] = {\"sequence\":read.query_sequence,\"stride\":mv_tag[0],\"mv_table\":np.array(mv_tag[1:]),\"num_trimmed\":ts_tag,\"shift\":sm_tag,\"scale\":sd_tag}\n",
    "    return np.array(seq_move, dtype=object)\n",
    "\n",
    "\n",
    "def read_from_pod5_bam(pod5_path, bam_path, read_id=None) -> np.array:\n",
    "    read = []\n",
    "    signal = extract_signal_from_pod5(pod5_path)\n",
    "    seq_move = extract_move_from_bam(bam_path)\n",
    "    if read_id is not None:\n",
    "        for i in range(len(seq_move)):\n",
    "            if seq_move[i][0] == read_id:\n",
    "                if seq_move[i][1] is not None:\n",
    "                    for j in range(len(signal)):\n",
    "                        if signal[j][0] == seq_move[i][0]:\n",
    "                            read.append(\n",
    "                                [\n",
    "                                    signal[j][0],\n",
    "                                    signal[j][1],\n",
    "                                    signal[j][2],\n",
    "                                    signal[j][3],\n",
    "                                    seq_move[i][1],\n",
    "                                    seq_move[i][2],\n",
    "                                    seq_move[i][3],\n",
    "                                    seq_move[i][4],\n",
    "                                    seq_move[i][5],\n",
    "                                    seq_move[i][6],\n",
    "                                ]\n",
    "                            )\n",
    "\n",
    "    else:\n",
    "        for i in range(len(seq_move)):\n",
    "            if seq_move[i][1] is not None:\n",
    "                for j in range(len(signal)):\n",
    "                    if signal[j][0] == seq_move[i][0]:\n",
    "                        read.append(\n",
    "                            [\n",
    "                                signal[j][0],\n",
    "                                signal[j][1],\n",
    "                                signal[j][2],\n",
    "                                signal[j][3],\n",
    "                                seq_move[i][1],\n",
    "                                seq_move[i][2],\n",
    "                                seq_move[i][3],\n",
    "                                seq_move[i][4],\n",
    "                                seq_move[i][5],\n",
    "                                seq_move[i][6],\n",
    "                            ]\n",
    "                        )\n",
    "                # 0:read_id,1:signal,2:to_pA_shift,3:to_pA_scale,4:sequence,5:stride,6:mv_table,7:num_trimmed,8:to_norm_shift,9:to_norm_scale\n",
    "\n",
    "    return np.array(read, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E::idx_find_and_load] Could not retrieve index file for '/homeb/xiaoyf/data/HG002/example/bam/has_moves.bam'\n",
      "/tmp/ipykernel_125167/3074756782.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(seq_move)\n"
     ]
    }
   ],
   "source": [
    "bam_path = \"/homeb/xiaoyf/data/HG002/example/bam/has_moves.bam\"\n",
    "bam = extract_move_from_bam(bam_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for read in bam:\n",
    "    # print(read[1])\n",
    "    if read[5] - int(read[5]) > 0:\n",
    "        print(\"shift is not int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod5_path = \"/homeb/xiaoyf/data/HG002/example/pod5/output.pod5\"\n",
    "signal = extract_signal_from_pod5(pod5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1230.,  945.,  993., ...,  687.,  696.,  846.], dtype=float16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signal_from_pod5_raw(pod5_path):\n",
    "    signals = []\n",
    "    with p5.Reader(pod5_path) as reader:\n",
    "        for read_record in reader.reads():\n",
    "            # signals[str(read_record.read_id)] = {'signal':read_record.signal,'shift':read_record.calibration.offset,'scale':read_record.calibration.scale}#不加str会变成UUID，很奇怪\n",
    "            signals.append(\n",
    "                [\n",
    "                    str(read_record.read_id),\n",
    "                    read_record.signal,\n",
    "                    read_record.calibration.offset,\n",
    "                    read_record.calibration.scale,\n",
    "                ]\n",
    "            )\n",
    "    return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33920\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.getsizeof(signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_raw = extract_signal_from_pod5_raw(pod5_path)\n",
    "# print(sys.getsizeof(signal_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for read in signal_raw:\n",
    "    # print(read[1])\n",
    "    if read[2] - int(read[2]) > 0:\n",
    "        print(\"shift is not int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "6\n",
      "80\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "my_list = [np.float16(1.0), np.float16(2.0), np.float16(3.0)]\n",
    "print(sys.getsizeof(my_list))\n",
    "my_array = np.array(my_list)\n",
    "print(my_array.nbytes)\n",
    "\n",
    "my_list = [1.0, 1.0, 1.0]\n",
    "print(sys.getsizeof(my_list))\n",
    "my_array = np.array(my_list)\n",
    "print(my_array.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bam_path = \"/homeb/xiaoyf/data/HG002/example/bam/sorted_has_moves.bam\"\n",
    "bamfile = pysam.AlignmentFile(bam_path, \"rb\", check_sq=False)\n",
    "bamfile.has_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for read in bamfile.get_index_statistics():\n",
    "    # print(read)\n",
    "    print(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for read in bamfile:\n",
    "    print(read.query_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_and_move_table = extract_move_from_bam(bam_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E::idx_find_and_load] Could not retrieve index file for '/homeb/xiaoyf/data/HG002/example/bam/has_moves.bam'\n"
     ]
    }
   ],
   "source": [
    "pod5_path = \"/homeb/xiaoyf/data/HG002/example/pod5/output.pod5\"\n",
    "bam_path = \"/homeb/xiaoyf/data/HG002/example/bam/has_moves.bam\"\n",
    "read = read_from_pod5_bam(pod5_path, bam_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3239b4d9-0a7e-471c-86fe-e156b9f279a0',\n",
       " array([1284, 1066, 1050, ..., 1008, 1023,  920], dtype=int16),\n",
       " -243.0,\n",
       " 0.1462070643901825,\n",
       " 'ATGTATATGTAACCTACTTGGTTCAGTTACGTACTGCTGAATAAGTCTCACAATATTGATGGCTTTAAAGAGAGAAGTCCCCGGCCGGGCTGGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCAAGGCAGGAGGATCACCTGAGGTCAAGAGTTCGAGACCAGCCTGGCCAACATGGTGAAACCTCCTCTCTACTAAAAATACAAAAATTAGCCAGGCATGGTGGCAGGTGCCTGTAATCCCAGCTACTTGGGAGGCTGAGGCAGGAGAATTGCTTGAACCTGGGAGGCAGAGATTGCAGTGAGCTGAGATCCCGCCACTGCAGTCCAGCCTGGGGGACAAGCAATACGTT',\n",
       " 5,\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 0], dtype=int8),\n",
       " 10,\n",
       " 101.37413024902344,\n",
       " 28.20626449584961]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature=caculate_feature_one_one_for_each_base(read,21)\n",
    "# feature['0000b1ad-fdaf-49e6-bc11-cbe93270e3a3'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_q = Queue()\n",
    "_prepare_read(read_q, read, batch_size=1000)\n",
    "read_batch = read_q.get()\n",
    "for read_id in read_batch:\n",
    "    print(read_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract_feature:   0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# caculate_bar=tqdm(total = read_number, desc='extract_feature', position=0)\n",
    "# write_feature_bar=tqdm(total = read_number, desc='write_feature', position=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iupac_alphabets = {\n",
    "    \"A\": [\"A\"],\n",
    "    \"T\": [\"T\"],\n",
    "    \"C\": [\"C\"],\n",
    "    \"G\": [\"G\"],\n",
    "    \"R\": [\"A\", \"G\"],\n",
    "    \"M\": [\"A\", \"C\"],\n",
    "    \"S\": [\"C\", \"G\"],\n",
    "    \"Y\": [\"C\", \"T\"],\n",
    "    \"K\": [\"G\", \"T\"],\n",
    "    \"W\": [\"A\", \"T\"],\n",
    "    \"B\": [\"C\", \"G\", \"T\"],\n",
    "    \"D\": [\"A\", \"G\", \"T\"],\n",
    "    \"H\": [\"A\", \"C\", \"T\"],\n",
    "    \"V\": [\"A\", \"C\", \"G\"],\n",
    "    \"N\": [\"A\", \"C\", \"G\", \"T\"],\n",
    "}\n",
    "iupac_alphabets_rna = {\n",
    "    \"A\": [\"A\"],\n",
    "    \"C\": [\"C\"],\n",
    "    \"G\": [\"G\"],\n",
    "    \"U\": [\"U\"],\n",
    "    \"R\": [\"A\", \"G\"],\n",
    "    \"M\": [\"A\", \"C\"],\n",
    "    \"S\": [\"C\", \"G\"],\n",
    "    \"Y\": [\"C\", \"U\"],\n",
    "    \"K\": [\"G\", \"U\"],\n",
    "    \"W\": [\"A\", \"U\"],\n",
    "    \"B\": [\"C\", \"G\", \"U\"],\n",
    "    \"D\": [\"A\", \"G\", \"U\"],\n",
    "    \"H\": [\"A\", \"C\", \"U\"],\n",
    "    \"V\": [\"A\", \"C\", \"G\"],\n",
    "    \"N\": [\"A\", \"C\", \"G\", \"U\"],\n",
    "}\n",
    "\n",
    "\n",
    "def get_refloc_of_methysite_in_motif(seqstr, motifset, methyloc_in_motif=0) -> list:\n",
    "    \"\"\"\n",
    "\n",
    "    :param seqstr:\n",
    "    :param motifset:\n",
    "    :param methyloc_in_motif: 0-based\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    motifset = set(motifset)\n",
    "    strlen = len(seqstr)\n",
    "    motiflen = len(list(motifset)[0])\n",
    "    sites = []\n",
    "    for i in range(0, strlen - motiflen + 1):\n",
    "        if seqstr[i : i + motiflen] in motifset:\n",
    "            sites.append(i + methyloc_in_motif)\n",
    "    return sites\n",
    "\n",
    "\n",
    "def _convert_motif_seq(ori_seq, is_dna=True):\n",
    "    outbases = []\n",
    "    for bbase in ori_seq:\n",
    "        if is_dna:\n",
    "            outbases.append(iupac_alphabets[bbase])\n",
    "        else:\n",
    "            outbases.append(iupac_alphabets_rna[bbase])\n",
    "\n",
    "    def recursive_permute(bases_list):\n",
    "        if len(bases_list) == 1:\n",
    "            return bases_list[0]\n",
    "        elif len(bases_list) == 2:\n",
    "            pseqs = []\n",
    "            for fbase in bases_list[0]:\n",
    "                for sbase in bases_list[1]:\n",
    "                    pseqs.append(fbase + sbase)\n",
    "            return pseqs\n",
    "        else:\n",
    "            pseqs = recursive_permute(bases_list[1:])\n",
    "            pseq_list = [bases_list[0], pseqs]\n",
    "            return recursive_permute(pseq_list)\n",
    "\n",
    "    return recursive_permute(outbases)\n",
    "\n",
    "\n",
    "def get_motif_seqs(motifs, is_dna=True):\n",
    "    ori_motif_seqs = motifs.strip().split(\",\")\n",
    "\n",
    "    motif_seqs = []\n",
    "    for ori_motif in ori_motif_seqs:\n",
    "        motif_seqs += _convert_motif_seq(ori_motif.strip().upper(), is_dna)\n",
    "    return motif_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(feature, index, nbase, nsig, num, fill_num=1):\n",
    "    nbase.append(np.tile(np.array(feature[index][5], dtype=str), num * fill_num))\n",
    "    # nstd.append(np.tile(feature[index][2],num*fill_num))\n",
    "    # nmean.append(np.tile(feature[index][3],num*fill_num))\n",
    "    try:\n",
    "        nsig.append(\n",
    "            np.tile(\n",
    "                np.random.choice(feature[index][1], size=num, replace=False), fill_num\n",
    "            )\n",
    "        )\n",
    "        # np.array取随机不能用random包，要用numpy自带的random\n",
    "    except Exception as e:\n",
    "        logger.critical(feature[index][1])\n",
    "    # return nbase,nsig\n",
    "\n",
    "\n",
    "# 0:read_id,1:signal,2:std,3:mean,4:num,5:base\n",
    "def _get_neighbord_feature(sequence, feature, base_num) -> list:\n",
    "    # 数据预处理主要速度瓶颈，同样的reads数，不运行这个函数大概快了十倍，从二十多分钟减到两分钟\n",
    "    motif = \"CG\"\n",
    "    max_sites = 15\n",
    "    motif_seqs = get_motif_seqs(motif)\n",
    "    tsite_locs = get_refloc_of_methysite_in_motif(sequence, set(motif_seqs))\n",
    "    if len(tsite_locs) > max_sites:\n",
    "        tsite_locs = np.random.choice(\n",
    "            tsite_locs,\n",
    "            size=max_sites,\n",
    "            replace=False,\n",
    "        )\n",
    "\n",
    "    nfeature = []\n",
    "    windows_size = (base_num - 1) // 2\n",
    "    signal_sample = 5\n",
    "    for i in range(len(feature)):\n",
    "        nbase = []\n",
    "        # nstd=[]\n",
    "        # nmean=[]\n",
    "        nsig = []\n",
    "        if i not in tsite_locs:\n",
    "            continue\n",
    "        # 更改扩增逻辑，增添采样函数\n",
    "        # remora一条read好像只提取15个点\n",
    "        # if feature[i][4]>base_num:\n",
    "        #    logger.info(\"base correspoding signal number {} is more than window size {}\".format(feature[i][4],base_num))\n",
    "\n",
    "        if i < windows_size:\n",
    "            flag = windows_size - i\n",
    "            expand(feature, i, nbase, nsig, signal_sample, windows_size - i)\n",
    "            if i != 0:\n",
    "                for k in range(i):  # 左闭右开\n",
    "                    expand(feature, k, nbase, nsig, signal_sample)\n",
    "                    flag += 1\n",
    "            for k in range(i, i + windows_size + 1):\n",
    "                expand(feature, k, nbase, nsig, signal_sample)\n",
    "                flag += 1\n",
    "            logger.debug(\n",
    "                \"focu base on the far left of read and expand number is {}\".format(flag)\n",
    "            )\n",
    "        elif (len(feature) - 1) - i < windows_size:\n",
    "            flag = 0\n",
    "            for k in range(i - windows_size, i + 1):\n",
    "                flag += 1\n",
    "                expand(feature, k, nbase, nsig, signal_sample)\n",
    "            if i != len(feature) - 1:\n",
    "                for k in range(i, len(feature) - 1):\n",
    "                    flag += 1\n",
    "                    expand(feature, k, nbase, nsig, signal_sample)\n",
    "            flag += windows_size - ((len(feature) - 1) - i)\n",
    "            expand(\n",
    "                feature,\n",
    "                i,\n",
    "                nbase,\n",
    "                nsig,\n",
    "                signal_sample,\n",
    "                windows_size - ((len(feature) - 1) - i),\n",
    "            )\n",
    "            logger.debug(\n",
    "                \"focu base on the far right of read and expand number is {}\".format(\n",
    "                    flag\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            for k in range(i - windows_size, i):\n",
    "                expand(feature, k, nbase, nsig, signal_sample)\n",
    "            for k in range(i, i + windows_size):\n",
    "                expand(feature, k, nbase, nsig, signal_sample)\n",
    "        # feature[read_id][i].update({'nbase':nbase,'nsig':nsig,'nstd':nstd,'nmean':nmean})\n",
    "        nfeature.append([feature[i][0], nbase, nsig])\n",
    "\n",
    "        # 0:read_id,1:nbase,2:nsig,3:nstd,4:nmean\n",
    "        # logger.debug('feature id: {}, feature:{}'.format(str(feature[0]),(str(nbase),str(nsig),str(nstd),str(nmean))))\n",
    "    return nfeature\n",
    "\n",
    "\n",
    "# 0:read_id,1:signal,2:to_pA_shift,3:to_pA_scale,4:sequence,5:stride,6:mv_table,7:num_trimmed,8:to_norm_shift,9:to_norm_scale\n",
    "def norm_signal_read_id(signal) -> np.array:\n",
    "    shift_scale_norm = []\n",
    "    # signal_norm=[]\n",
    "    if signal[3] == 0:\n",
    "        logger.critical(\"to_pA_scale of read {} is 0\").format(signal[0])\n",
    "    shift_scale_norm = [\n",
    "        (signal[8] / signal[3]) - np.float16(signal[2]),\n",
    "        (signal[9] / signal[3]),\n",
    "    ]\n",
    "    # 0:shift,1:scale\n",
    "    num_trimmed = signal[7]\n",
    "    # print('num_trimmed:{} and signal:{}'.format(num_trimmed,signal[1]))\n",
    "    # print('shift:{} and scale:{}'.format(shift_scale_norm[0],shift_scale_norm[1]))\n",
    "    if shift_scale_norm[1] == 0:\n",
    "        logger.critical(\"scale of read {} is 0\").format(signal[0])\n",
    "    if num_trimmed >= 0:\n",
    "        signal_norm = (\n",
    "            signal[1][num_trimmed:].astype(np.float16) - shift_scale_norm[0]\n",
    "        ) / shift_scale_norm[1]\n",
    "    else:\n",
    "        signal_norm = (\n",
    "            signal[1][:num_trimmed].astype(np.float16) - shift_scale_norm[0]\n",
    "        ) / shift_scale_norm[1]\n",
    "\n",
    "    return signal_norm\n",
    "\n",
    "\n",
    "def caculate_batch_feature_for_each_base(\n",
    "    lock, read_q, feature_q, base_num=0, write_batch=10\n",
    "):\n",
    "    # print(\"extrac_features process-{} starts\".format(os.getpid()))\n",
    "    logger.info(\"extrac_features process-{} starts\".format(os.getpid()))\n",
    "    read_num = 0\n",
    "\n",
    "    while True:\n",
    "        if read_q.empty():\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "        lock.acquire()\n",
    "        read_batch = read_q.get()\n",
    "        lock.release()\n",
    "        if read_batch == \"kill\":\n",
    "            read_q.put(\"kill\")\n",
    "            # time.sleep(10)\n",
    "            break\n",
    "        read_num += len(read_batch)\n",
    "        # flag=0\n",
    "        # if len(read_batch)>1:\n",
    "        #    flag=1\n",
    "        #    pos=bar_q.get()\n",
    "        #    caculate_bar = tqdm(total = len(read_batch), desc='extract_feature', position=pos)\n",
    "        #    bar_q.put(pos+1)\n",
    "        # else:\n",
    "        #    flag=0\n",
    "        logger.info(\"read batch size: {}\".format(len(read_batch)))\n",
    "        nfeature = []\n",
    "        for read_one in read_batch:\n",
    "            feature = []\n",
    "            #    if flag == 1:\n",
    "            #        caculate_bar.update()\n",
    "            # print(read_one)\n",
    "            sequence = read_one[4]  # 这个转成np.array内存占用大很多\n",
    "            stride = read_one[5]\n",
    "            movetable = np.array(read_one[6])\n",
    "            # num_trimmed = read[read_id]['num_trimmed']\n",
    "            trimed_signals = norm_signal_read_id(read_one)  # 筛掉背景信号,norm\n",
    "            if trimed_signals.size == 0:\n",
    "                logger.critical(\"norm has error, raw data is {}\".format(read_one))\n",
    "                continue\n",
    "            move_pos = np.append(np.argwhere(movetable == 1).flatten(), len(movetable))\n",
    "            # print(len(move_pos))\n",
    "\n",
    "            for move_idx in range(len(move_pos) - 1):\n",
    "                start, end = move_pos[move_idx], move_pos[move_idx + 1]\n",
    "                signal = trimed_signals[(start * stride) : (end * stride)]  # .tolist()\n",
    "                if signal.size == 0:\n",
    "                    logger.critical(\n",
    "                        \"signal is empty, it's crazy, read id is {} and base index is\".format(\n",
    "                            read_one[0], move_idx\n",
    "                        )\n",
    "                    )\n",
    "                    continue\n",
    "                if True in np.isnan(signal):\n",
    "                    logger.critical(\"signal has nan for read_id:{}\".format(read_one[0]))\n",
    "\n",
    "                try:\n",
    "                    mean = np.mean(signal)\n",
    "                    if np.amax(signal) < mean:\n",
    "                        logger.critical(\n",
    "                            \"ValueERROR: mean greater than max for read_id:{}\".format(\n",
    "                                read_one[0]\n",
    "                            )\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    logger.critical(signal)\n",
    "                std = np.std(signal.astype(np.float32))  # np.float16会溢出\n",
    "                num = end - start\n",
    "\n",
    "                feature.append(\n",
    "                    [\n",
    "                        read_one[0],\n",
    "                        signal,\n",
    "                        np.float16(std),\n",
    "                        np.float16(mean),\n",
    "                        np.int8(num * stride),\n",
    "                        sequence[move_idx],\n",
    "                    ]\n",
    "                )\n",
    "                # 0:read_id,1:signal,2:std,3:mean,4:num,5:base\n",
    "                # feature[read_id].append({'signal':signal,'std':str(std),'mean':str(mean),'num':int(num*stride),'base':sequence[move_idx]})\n",
    "            if base_num != 0:\n",
    "                nfeature.append(_get_neighbord_feature(sequence, feature, base_num))\n",
    "                logger.info(\n",
    "                    \"extract neigbor features for read_id:{}\".format(read_one[0])\n",
    "                )\n",
    "                if len(nfeature) == write_batch:\n",
    "                    lock.acquire()\n",
    "                    feature_q.put(nfeature)\n",
    "                    lock.release()\n",
    "                    nfeature = []\n",
    "                    while feature_q.qsize() > 50:\n",
    "                        time.sleep(2)\n",
    "                        if feature_q.full():\n",
    "                            logger.error(\"queue full\")\n",
    "\n",
    "            # feature_q.put(feature)\n",
    "        if len(nfeature) != 0:\n",
    "            lock.acquire()\n",
    "            feature_q.put(nfeature)\n",
    "            lock.release()\n",
    "            nfeature = []\n",
    "        # feature_q.append(feature)\n",
    "\n",
    "        # print(\"extrac_features process-{} ending, proceed {} read batch\".format(os.getpid(), read_num))\n",
    "    logger.info(\n",
    "        \"extrac_features process-{} ending, proceed {} read\".format(\n",
    "            os.getpid(), read_num\n",
    "        )\n",
    "    )\n",
    "    # if caculate_bar is not None:\n",
    "    #    caculate_bar.close()\n",
    "    # pbar.close()\n",
    "\n",
    "\n",
    "def _prepare_read(read_q, read, batch_size=1000):\n",
    "    i = 0\n",
    "    # j=0\n",
    "    read_batch = []\n",
    "    for read_one in read:\n",
    "        read_batch.append(read_one)\n",
    "        i = i + 1\n",
    "        # j=j+1\n",
    "        # if j==40:\n",
    "        #    break\n",
    "        if i == batch_size:\n",
    "            i = 0\n",
    "            if read_q.full():\n",
    "                logger.critical(\"read_q is full\")\n",
    "            read_q.put(read_batch)\n",
    "            read_batch = []\n",
    "    read_q.put(np.array(read_batch, dtype=object))\n",
    "    # print('total batch number is {}'.format((len(read)-1)//batch_size+1))\n",
    "    logger.info(\"total batch number is {}\".format((len(read) - 1) // batch_size + 1))\n",
    "    # return len(read)\n",
    "\n",
    "\n",
    "def write_feature(read_number, file, feature_q):\n",
    "    # print(\"write_process-{} starts\".format(os.getpid()))\n",
    "    logger.info(\"write_process-{} starts\".format(os.getpid()))\n",
    "    dataset = []\n",
    "    # pos=bar_q.get()\n",
    "    write_feature_bar = tqdm(\n",
    "        total=read_number,\n",
    "        desc=\"extract feature\",\n",
    "        position=0,\n",
    "        colour=\"green\",\n",
    "        unit=\" read\",\n",
    "    )\n",
    "    # bar_q.put(pos+1)\n",
    "    try:\n",
    "        with open(\"/home/xiaoyf/methylation/deepsignal/log/feature.txt\", \"w\") as f:\n",
    "            while True:\n",
    "                if feature_q.empty():\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                write_batch = feature_q.get()\n",
    "                if write_batch == \"kill\":\n",
    "                    logger.info(\"write_process-{} finished\".format(os.getpid()))\n",
    "                    # time.sleep(10)\n",
    "                    np_data = np.array(dataset)\n",
    "                    np.save(\"/home/xiaoyf/methylation/deepsignal/log/data.npy\", np_data)\n",
    "                    # 包含neigbor feature的40条reads保存成npy需要27.87GB，这个开销是无法忍受的\n",
    "                    # print('write_process-{} finished'.format(os.getpid()))\n",
    "                    break\n",
    "\n",
    "                # logger.debug('feature id: {}'.format(str(features[0][0])))\n",
    "                for read in write_batch:\n",
    "                    write_feature_bar.update()\n",
    "                    logger.info(\n",
    "                        \"write process get neigbor features number:{}\".format(len(read))\n",
    "                    )\n",
    "                    for feature in read:\n",
    "                        # 0:read_id,1:nbase,2:nsig,3:nstd,4:nmean\n",
    "                        # #f.write(read_id+'\\t')\n",
    "\n",
    "                        dataset.append(feature)\n",
    "                        f.write(\n",
    "                            str(feature[0])\n",
    "                            + \"\\t\"\n",
    "                            + str(feature[1])\n",
    "                            + \"\\t\"\n",
    "                            + str(feature[2])\n",
    "                            + \"\\n\"\n",
    "                        )\n",
    "\n",
    "                f.flush()\n",
    "    except Exception as e:\n",
    "        logger.critical(\n",
    "            \"error in writing features, this always happend because memory not enough\"\n",
    "        )\n",
    "        print(e)\n",
    "    finally:\n",
    "        write_feature_bar.close()\n",
    "\n",
    "\n",
    "def bar_listener(p_bar, desc=\"\", position=1, number=4000):\n",
    "    bar = tqdm(total=number, desc=desc, position=position)\n",
    "    for item in iter(p_bar.get, None):\n",
    "        bar.update(item)\n",
    "\n",
    "\n",
    "def extract_feature(read, nproc=4, batch_size=20):\n",
    "    start = time.time()\n",
    "    feature_q = Queue()\n",
    "    read_q = Queue()\n",
    "    # bar=Queue()\n",
    "    # bar.put(0)\n",
    "    # caculate_batch_feature_pbar = Manager().Queue()\n",
    "    # write_pbar = Manager().Queue()\n",
    "    _prepare_read(read_q, read, batch_size)\n",
    "    read_number = len(read)\n",
    "    feature_procs = []\n",
    "    read_q.put(\"kill\")\n",
    "    manager = mp.Manager()\n",
    "    lock = manager.Lock()  # 初始化一把锁\n",
    "\n",
    "    # extract_feature_bar = mp.Process(target=bar_listener, args=(caculate_batch_feature_pbar, \"extract_features\", 1,))\n",
    "    # extract_feature_bar.daemon = True\n",
    "    # extract_feature_bar.start()\n",
    "\n",
    "    for _ in range(nproc):\n",
    "        p = mp.Process(\n",
    "            target=caculate_batch_feature_for_each_base,\n",
    "            args=(\n",
    "                lock,\n",
    "                read_q,\n",
    "                feature_q,\n",
    "                21,\n",
    "            ),\n",
    "        )\n",
    "        p.daemon = True\n",
    "        p.start()\n",
    "        feature_procs.append(p)\n",
    "\n",
    "    write_filename = \"/home/xiaoyf/methylation/deepsignal/log/data.npy\"\n",
    "\n",
    "    # write_feature_bar = mp.Process(target=bar_listener, args=(write_pbar, \"write_features\", 2,))\n",
    "    # write_feature_bar.daemon = True\n",
    "    # write_feature_bar.start()\n",
    "    # tqdm(total = 4000, desc=\"write_features\", position=1)\n",
    "\n",
    "    p_w = mp.Process(\n",
    "        target=write_feature,\n",
    "        args=(\n",
    "            read_number,\n",
    "            write_filename,\n",
    "            feature_q,\n",
    "        ),\n",
    "    )\n",
    "    p_w.daemon = True\n",
    "    p_w.start()\n",
    "    # with tqdm(total = read_number, desc='extract_feature', position=0) as pbar:\n",
    "    for p in feature_procs:\n",
    "        p.join()\n",
    "\n",
    "    # caculate_bar.close()\n",
    "    # while True:\n",
    "    #    flag=0\n",
    "    #    for p in feature_procs:\n",
    "    #        if not p.is_alive():\n",
    "    #            flag+=1\n",
    "    #    if flag==0:\n",
    "    #        break\n",
    "    #    if flag!=0 and not p_w.is_alive():\n",
    "    #        logger.error(\"p_w terminate error\")\n",
    "    #        p_w.join()\n",
    "    #        p_w.start()\n",
    "    while True:\n",
    "        flag = 0\n",
    "        for p in feature_procs:\n",
    "            if p.is_alive():\n",
    "                flag += 1\n",
    "        if flag == 0:\n",
    "            break\n",
    "    feature_q.put(\"kill\")\n",
    "    p_w.join()\n",
    "    # write_feature_bar.close()\n",
    "\n",
    "    # extract_feature_bar.join()\n",
    "    # write_feature_bar.join()\n",
    "    # print(\"[main]extract_features costs %.1f seconds..\" %(time.time() - start))\n",
    "    logger.info(\"[main]extract_features costs %.1f seconds..\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract feature:   8%|\u001b[32m▊         \u001b[0m| 320/4000 [04:04<08:12,  7.47 read/s]  "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extract_feature(read, 2, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -86  108   96 ... -126  118   36]\n",
      "76421\n",
      "10111\n",
      "-6\n",
      "[-4.133 -3.11  -3.172 ... -4.33  -3.008 -3.06 ]\n",
      "76415\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "neg = 0\n",
    "for r in read:\n",
    "    if r[7] < 0:\n",
    "        neg += 1\n",
    "    if r[0] == \"73e5c496-c9c6-42e4-8150-9cdb5ad83c52\":\n",
    "        print(r[1])\n",
    "        print(len(r[1]))\n",
    "        print(len(r[4]))\n",
    "        print(r[7])\n",
    "        focus = r\n",
    "norm_signal = norm_signal_read_id(focus)\n",
    "print(norm_signal)\n",
    "print(len(norm_signal))\n",
    "print(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0:read_id,1:signal,2:to_pA_shift,3:to_pA_scale,4:sequence,5:stride,6:mv_table,7:num_trimmed,8:to_norm_shift,9:to_norm_scale\n",
    "read_q = Queue()\n",
    "_prepare_read(read_q, read, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.getsizeof(np.array([\"a\", \"b\", \"s\", \"d\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([1, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(\"absd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"asvde\"[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_batch = read_q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['910762d9-7962-43db-bbea-556bddc10e69'\n",
      " array([ 59, -22, -27, ...,  94, 104, -11], dtype=int8) 19 0.1462\n",
      " 'ATGTGTTACTTCGTTCAGTTACGTATTGCTTTTTTAATTTATTTAGGGGGAATGATGGTTGTCTTTGGATATACTACAGCGATGGCTATTGAGGAGTATCCTGAGGCATGGGGGTCAGGGGTTGAGGTCTTGGTGAGTGTTTTAGTGGGGTTAGCGATGGAGGTAGGATTGGTGCTGTGGGTGAAAGAGTATGATGGGGTGGTGGTTGTGGTAAACTTTAATAGTGTAGGAAGCTGAATAAACTTATGAAGGAGGGGTCAGGGTTGATTCGGGAGGATCCTATTGGTGCGGGGGCTTTGTATGATTATGGGCGTTGATTAGTAGTAGTTACTGGTTGAACATTGTTTGTTGGTGTATATATTGTAATTGAGATTGCTGGGGGAATAGGTTATGTGATTAGGAGTAGGGTTAGGATGAGTGGGAAGAAGAAAGAGAGGAAGTAAAGTTTAATTATGCCTTTTTGGGTTGAGGTGATGATGGAGGTGGAGATTTGGTGCTGTGAAATTGTTTTAGGTAATAGCTTTTCTAGTCAGGTTAGGTCTAGGGGGAGTAGGGGCAGGTTTTGGCTCGTAAGAAGGCCTGGGTGGGATTGTGCGGTGTGTGATGCTGGGTAGAATGCGAGTATGTTGGAGAAATAAAATGTGCATAGTGGGGATTTTATTTTAAGTTTGTTGGTTAGGTAGTTGAGGTCTAGGGCTGTTAGAAGTCCTAGGAAAGTGACAGCGAGGGCTGTGAGTTTTAGGTAGAGATGTTGGAAGGGGGATGCGGGGGAAATGTTGTTAGTAATGAGAAATCCTAGGAATACGGCTTTTCCGGCTGCCAGGCGTTTAATGGGGTTTAGTAGGGTGGGGTTATTTTCGTTAATGTTAGTAAGGGTGGGGAAGCGAGGTTGACCTGTTAGGGTGAGAAGAATTATTCGAGTGCTATAGGCGCTTGTCAGGGAGGTAGCGATGGAGTAATAGATAGGGCTCAGGCGTTTGTGTATGATATGTTTGCGGTTTCGATGATGTGGTCTTTGGAGTAGAAACCTGTGAGGAAAGGTATTCCTGCTAATGCTAGGCTGCCAATGGAAGGGAGTTTGAAGTGAGAGGTATGGTTTTGAGTAGTCCTCCTATTTTAGGAATATCTTTGTTCATTGTTAAGGTTGTGGATGATGGACCCGGAGCACATAAATAGTATGGCTTTGAAGAAGGCGTGGGTACAGATGTGCAGGAATGCTAGGTGTGGTTGGTTGATGCCGATTGTAACTATTATGAGTCCTAGTTGACTTTGAAATGGGGGAGGGTTACGATTTTGATGTCATTTTGTGTAAGGGCGCAGACTGCTGCGAACAGAGTGGTGATAGCGCCTAAGCATAGTGTTAGAGTTTGGATTAGTGGGCTATTTTCTGCTAGGGGGTGGAAGCAGATAAGTAAGAAGATTCCTGCTAAGACTATAGTGCTTGAGTGGAGTAGGGCTGAGACTGGGGTGGGGGCCTTCTATGGCTGAGGGGAGTCAGGTAGAACCTAATTGGGCTGATTTGCCTGCTGCTGCTTGGGGGAAAGTACACATTCATTGAGGTGAGGCCGCTACACATTTAGAAGGGCTATTTGTTGTGGGTCTCATGAGTTGGAGTGTAGGATAAATCATGCTAAGGCGAGGATGAAACCGATATCGCCGATACGGTTGTATAGGATTGCTTGAATGGCTGCTGTGTTGGCATACTGCTTCGGGCGTATCATCAACTGATGAGCAAGAAGGATATAATTCCTACGCCCTCTCAGCCGATGAACAGTTGGAATAGGTTGTTAAGCGGTAGTTCGATTATTAGTATGGTAATTGGGAGATGGAGTCATATTTGAAGAACTGATTAATGTTTGGGTCTGAGTTTATATATCACAGTGAGAATTCTATGATGGACCATGTAACGAACAATGCTACAGGGATGAATATTATGGAGAAGTAGTCTCATTTTGAAGCTAAGGGAGGCTGGGTTGTTTGGGTTATCGTCAGTAGATGTCGATAATAACTTCTTGGTCTAGGCACATGAATATTGTTGTGGGAGGGGCTGGATGTAATAAAGGTGGATGCGACAATGGATTTTACATAATGGGGGTATGAGTTTTTTTTTGTTAGGGTTAACAGAGGTGGTGGGGGCGATTGGGAAGTCAGGGTTAGGGTGGTTATCGTGGTGTCATGGTTATTACTTTTCTTGGAGTTGCACCAAATATTTTGGGGCCTAGACCAATGGATAGCTGTTATCCTTTAAAAGTTGAGAAAGCCATGTTGTTAGACATGGGGGCATGAGTTTAGCAGTTCTTGTGACTTTATTCGGTGAATAAGGGGTCGTAAGCCTCTGTTGTCAGATTCACAATCTGATGTTTTGGTTAAACTATATTTCACAAGAGGAAAACCCGGTAATGATGTCGGGGTTGAGGGATAGGAGGAGAATGGGGGATAGGTGTATGGACGTGGGGTGTTTCTCGTGTGAATGAGGGTTTTATGTTGTTAATGTGGTGGGTGAGTGAGCCCCATTGTGTTGTGGTAAATCTAGAGGATATAGGGCTGTGACTAGTATGTTGAGTCCTGTAAGTGGAGAGTGATATTTGATGGGGAGGCGTTGGTTACTGGCACAGAGAGTTCTCCCAGTAGGTTAATAGGTGGGGGTAAGGCAAGGACACGAGGCTTGTAGAAGTCATCAAAAAGCTATCCATGGGAGATCTAGACTTGAAGTCCTTGAGAGGATTATGATGCGACTGTGCATGCATTCATGATTTGAGTTTGCTAGGCAAGATAGTAATGAGGATGTAAGCCCATAGGCAGATGTCAGAATGACTGCACGGTGAAGCTAGTGGGTTTGGATGAGAATGGCTATTACTGCTGGGCTATGTGGCTTAGTAAAAGTATTGCAATAGAGCCATTTGGTGTGTTTGGTGGGCAGATGACATTGTTACTGACTTGCTACATGGATGATGGAGGGTTTGTTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGGTTGGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGGCGGGTGTAAGCAATATACTAGTACTCCTAGAAGTGAGATGGTAAATGCTAGTATAATATTTATGTAAATGAGGGGCATTTGGTAAATATGATTATCATAATTTAATGAGTGGAATCATTCGTTTTATAAACTATCATACCAATTCGGTTCAGTCTAATCCTTTTTGTAGTCACTCTCACTGCCCAAGAACTATCGAACTCCTGAGCCAACAACTTAATATGACTAGCTTACACAATAGCTTCTTATAGTAAAGATACCTCTTACGGACTCCACTTATGACTCCCTAAAGCCCATGTCGAAGCCCCCATGGCTGGGGTCAATAGTACTTGCCGCAGTACTCTTGAAACTAGGCGGCTATGGTATAATACACCTCACACTCATTCTCAACCCCCTGACAAAACACATAGCCTACCCCTTCCTTGTACTCCCTATGAGGCGTGACTGTAACAAGCTCCATCTGCCTACGACAAACAGACCTAAAATCGCTCATTGCATACTCTTCAATCAGCCACATAGCCCTCGTAGTAACAGCCATTCTCATCCAAACCCCCTGAAGCTTCACCGGCGCAGTCATTCTCATAATCGCCCACGGGCTTACATCCTCATTACTATTCTGCCTAGCAAACTCAAACTACGAACGCACTCACAGTCGCATCATAATCCTCTACTCAAGGACTTCAAACTCTACTCCCACTAATGACTTTTCAATAACTCCTGGCAAGCCCTGGCTGACCTCGCCT'\n",
      " 5 array([1, 0, 0, ..., 1, 1, 1], dtype=int8) 10 111.44 29.98]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for read_i in read_batch:\n",
    "    if i == 1:\n",
    "        print(read_i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(nproc=4):\n",
    "    start = time.time()\n",
    "    feature_q = Queue()\n",
    "    read_q = Queue()\n",
    "    _prepare_read(read_q, read, batch_size=20)\n",
    "    feature_procs = []\n",
    "    read_q.put(\"kill\")\n",
    "    caculate_batch_feature_pbar = Queue()\n",
    "    extract_feature_bar = mp.Process(\n",
    "        target=bar_listener,\n",
    "        args=(\n",
    "            caculate_batch_feature_pbar,\n",
    "            \"extract_features\",\n",
    "            1,\n",
    "        ),\n",
    "    )\n",
    "    extract_feature_bar.daemon = True\n",
    "    extract_feature_bar.start()\n",
    "    for _ in range(nproc):\n",
    "        p = mp.Process(\n",
    "            target=caculate_batch_feature_for_each_base,\n",
    "            args=(\n",
    "                caculate_batch_feature_pbar,\n",
    "                read_q,\n",
    "                feature_q,\n",
    "                21,\n",
    "            ),\n",
    "        )\n",
    "        p.daemon = True\n",
    "        p.start()\n",
    "        feature_procs.append(p)\n",
    "\n",
    "    write_filename = \"/home/xiaoyf/methylation/deepsignal/log/feature.txt\"\n",
    "    write_pbar = Queue()\n",
    "    write_feature_bar = mp.Process(\n",
    "        target=bar_listener,\n",
    "        args=(\n",
    "            write_pbar,\n",
    "            \"write_features\",\n",
    "            2,\n",
    "        ),\n",
    "    )\n",
    "    write_feature_bar.daemon = True\n",
    "    write_feature_bar.start()\n",
    "    # tqdm(total = 4000, desc=\"write_features\", position=1)\n",
    "    p_w = mp.Process(\n",
    "        target=write_feature,\n",
    "        args=(\n",
    "            write_pbar,\n",
    "            write_filename,\n",
    "            feature_q,\n",
    "        ),\n",
    "    )\n",
    "    p_w.daemon = False\n",
    "    p_w.start()\n",
    "\n",
    "    for p in feature_procs:\n",
    "        p.join()\n",
    "\n",
    "    feature_q.put(\"kill\")\n",
    "    p_w.join()\n",
    "    write_pbar.put(None)\n",
    "    caculate_batch_feature_pbar.put(None)\n",
    "    extract_feature_bar.join()\n",
    "    write_feature_bar.join()\n",
    "    # print(\"[main]extract_features costs %.1f seconds..\" %(time.time() - start))\n",
    "    logger.info(\"[main]extract_features costs %.1f seconds..\" % (time.time() - start))\n",
    "\n",
    "\n",
    "def write_feature(file, feature_q):\n",
    "    # print(\"write_process-{} starts\".format(os.getpid()))\n",
    "    logger.info(\"write_process-{} starts\".format(os.getpid()))\n",
    "    dataset = []\n",
    "    try:\n",
    "        with open(file, \"w\") as f:\n",
    "            while True:\n",
    "                if feature_q.empty():\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                features = feature_q.get()\n",
    "                if features == \"kill\":\n",
    "                    logger.info(\"write_process-{} finished\".format(os.getpid()))\n",
    "                    np_data = np.array(dataset)\n",
    "                    np.save(\"/home/xiaoyf/methylation/deepsignal/log/data.npy\", np_data)\n",
    "                    # 包含neigbor feature的40条reads保存成npy需要27.87GB，这个开销是无法忍受的\n",
    "                    # print('write_process-{} finished'.format(os.getpid()))\n",
    "                    break\n",
    "                logger.info(\n",
    "                    \"write process get neigbor features number:{}\".format(len(features))\n",
    "                )\n",
    "                # logger.debug('feature id: {}'.format(str(features[0][0])))\n",
    "                for feature in features:\n",
    "                    # 0:read_id,1:nbase,2:nsig,3:nstd,4:nmean\n",
    "                    # f.write(read_id+'\\t')\n",
    "                    write_feature_bar.update()\n",
    "                    dataset.append(feature[1:])\n",
    "                    # f.write(str(feature[1])+'\\t'+str(feature[2])+\n",
    "                    #            '\\t'+str(feature[3])+'\\t'+str(feature[4])+'\\n')\n",
    "\n",
    "                f.flush()\n",
    "    except Exception as e:\n",
    "        logger.error(\"error in writing features\")\n",
    "        print(e)\n",
    "    # write_pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:40<00:00,  9.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "def listener(q):\n",
    "    pbar = tqdm(total=1000)\n",
    "    while True:\n",
    "        if not q.empty():\n",
    "            k = q.get()\n",
    "            if k == 1:\n",
    "                pbar.update(1)\n",
    "            else:\n",
    "                break\n",
    "    pbar.close()\n",
    "\n",
    "\n",
    "def solve(q):\n",
    "    for i in range(100):\n",
    "        time.sleep(1)\n",
    "        q.put(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    manage = mp.Manager()\n",
    "    q = manage.Queue()\n",
    "    p = mp.Process(target=listener, args=(q,))\n",
    "    p.start()\n",
    "    processList = []\n",
    "    for i in range(10):\n",
    "        t = mp.Process(target=solve, args=(q,))\n",
    "        processList.append(t)\n",
    "        t.start()\n",
    "    for t in processList:\n",
    "        t.join()\n",
    "    q.put(-1)\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/home/xiaoyf/methylation/deepsignal/log/data.npy\", allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#0:read_id,1:nbase,2:nsig,3:nstd,4:nmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-0.111112-0.111112'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"-0.111112\" * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.111112, -0.111112]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[-0.111112] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['732722e8-65c8-4ad1-b4d5-f9ec85e830d3',\n",
       "       list([['A', 'A', 'A', 'A', 'A'], ['G', 'G', 'G', 'G', 'G'], ['C', 'C', 'C', 'C', 'C'], ['A', 'A', 'A', 'A', 'A'], ['G', 'G', 'G', 'G', 'G'], ['A', 'A', 'A', 'A', 'A'], ['G', 'G', 'G', 'G', 'G'], ['A', 'A', 'A', 'A', 'A'], ['G', 'G', 'G', 'G', 'G'], ['A', 'A', 'A', 'A', 'A'], ['C', 'C', 'C', 'C', 'C'], ['G', 'G', 'G', 'G', 'G'], ['G', 'G', 'G', 'G', 'G'], ['G', 'G', 'G', 'G', 'G'], ['G', 'G', 'G', 'G', 'G'], ['T', 'T', 'T', 'T', 'T'], ['T', 'T', 'T', 'T', 'T'], ['T', 'T', 'T', 'T', 'T'], ['G', 'G', 'G', 'G', 'G'], ['G', 'G', 'G', 'G', 'G']]),\n",
       "       list([[-0.90380859375, -0.98291015625, -0.75, -0.931640625, -1.048828125], [-0.86181640625, -0.9130859375, -0.98779296875, -0.94140625, -0.98779296875], [-1.0205078125, -1.052734375, 0.228271484375, -0.69873046875, -0.92724609375], [-0.4658203125, -0.80126953125, -0.796875, -0.89453125, -0.810546875], [-1.11328125, -1.169921875, -1.1279296875, -1.1923828125, -0.9921875], [-1.0673828125, -1.015625, -1.099609375, -1.08984375, -0.98291015625], [-1.109375, -0.96435546875, -1.146484375, -1.052734375, -1.0859375], [-1.2861328125, -1.2021484375, -1.28125, -1.18359375, -1.3701171875], [-0.9970703125, -1.1787109375, -1.3095703125, -1.052734375, -1.13671875], [-1.150390625, -1.052734375, -1.146484375, -1.1787109375, -1.13671875], [-1.173828125, -1.123046875, 0.1444091796875, -1.0673828125, -1.20703125], [0.2841796875, 0.251708984375, -0.78271484375, 0.1910400390625, 0.1864013671875], [-1.052734375, -0.82958984375, -1.0205078125, -1.13671875, -1.0712890625], [-1.1416015625, -1.1044921875, -1.173828125, -1.0625, -1.029296875], [-1.1279296875, -1.0390625, -1.029296875, -0.92236328125, -0.955078125], [-0.82958984375, -1.169921875, -1.2578125, -0.8388671875, -0.4287109375], [0.65673828125, 0.591796875, 0.736328125, 0.74560546875, 0.68505859375], [1.3232421875, 0.78271484375, 0.615234375, 0.6708984375, 1.234375], [-1.048828125, 0.409912109375, -1.08984375, -1.267578125, 0.470703125], [-0.84814453125, -1.3740234375, -1.048828125, -1.0673828125, -0.92724609375]]),\n",
       "       list([0.49896240234375, 0.281829833984375, 2.5830078125, 0.59112548828125, 0.32745361328125, 0.1983642578125, 0.2899169921875, 0.33416748046875, 0.53924560546875, 0.3448486328125, 2.0166015625, 2.030029296875, 0.5810546875, 0.2606201171875, 0.35858154296875, 1.295166015625, 0.280914306640625, 1.573486328125, 2.288818359375, 0.897216796875]),\n",
       "       list([-4.61669921875, -4.66552734375, -3.1201171875, -4.00146484375, -5.7373046875, -5.185546875, -5.4638671875, -6.3232421875, -5.673828125, -5.3369140625, -4.65087890625, 0.130615234375, -4.94384765625, -5.5126953125, -5.0732421875, -5.1806640625, 3.41552734375, 4.31396484375, -4.59716796875, -5.263671875])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3239b4d9-0a7e-471c-86fe-e156b9f279a0',\n",
       "       list([0.8430438164710445, 0.8067593431803018, 0.7963923508115183, 0.7704748698895593, 0.8274933279178691]),\n",
       "       '0.025095831333870715', '0.8088327416540585', 5, 'T'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "all_kmer_levels = dict((\"\".join(bs), []) for bs in product(\"ACGT\", repeat=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base2code_dna = {\n",
    "    \"A\": 0,\n",
    "    \"C\": 1,\n",
    "    \"G\": 2,\n",
    "    \"T\": 3,\n",
    "    \"N\": 4,\n",
    "    \"W\": 4,\n",
    "    \"S\": 4,\n",
    "    \"M\": 4,\n",
    "    \"K\": 4,\n",
    "    \"R\": 4,\n",
    "    \"Y\": 4,\n",
    "    \"B\": 4,\n",
    "    \"V\": 4,\n",
    "    \"D\": 4,\n",
    "    \"H\": 4,\n",
    "    \"Z\": 4,\n",
    "}  # set 4 for all bases except ACGT, for now\n",
    "\n",
    "\n",
    "def norm_signal_read_id_dict(signal):\n",
    "    shift_scale_norm = {}\n",
    "    signal_norm = {}\n",
    "    shift_scale_norm = {}\n",
    "    shift_scale_norm[\"shift\"] = (\n",
    "        signal[\"to_norm_shift\"] / signal[\"to_pA_scale\"]\n",
    "    ) - signal[\"to_pA_shift\"]\n",
    "    shift_scale_norm[\"scale\"] = signal[\"to_norm_scale\"] / signal[\"to_pA_scale\"]\n",
    "    num_trimmed = signal[\"num_trimmed\"]\n",
    "    signal_norm = (\n",
    "        signal[\"signal\"][num_trimmed:] - shift_scale_norm[\"shift\"]\n",
    "    ) / shift_scale_norm[\"scale\"]\n",
    "    return signal_norm\n",
    "\n",
    "\n",
    "def read_from_pod5_bam_dict(pod5_path, bam_path, read_id=None):\n",
    "    read = {}\n",
    "    signal = extract_signal_from_pod5(pod5_path)\n",
    "    seq_move = extract_move_from_bam(bam_path)\n",
    "    if read_id is not None:\n",
    "        if seq_move[read_id][\"sequence\"] is not None:\n",
    "            if signal[read_id] is not None:\n",
    "                read[read_id] = {\n",
    "                    \"sequence\": seq_move[read_id][\"sequence\"],\n",
    "                    \"signal\": signal[read_id][\"signal\"],\n",
    "                    \"mv_table\": seq_move[read_id][\"mv_table\"],\n",
    "                    \"num_trimmed\": seq_move[read_id][\"num_trimmed\"],\n",
    "                    \"to_norm_shift\": seq_move[read_id][\"shift\"],\n",
    "                    \"to_norm_scale\": seq_move[read_id][\"scale\"],\n",
    "                    \"stride\": seq_move[read_id][\"stride\"],\n",
    "                    \"to_pA_shift\": signal[read_id][\"shift\"],\n",
    "                    \"to_pA_scale\": signal[read_id][\"scale\"],\n",
    "                }\n",
    "    else:\n",
    "        for read_id in seq_move.keys():\n",
    "            if seq_move[read_id][\"sequence\"] is not None:\n",
    "                if signal[read_id] is not None:\n",
    "                    read[read_id] = {\n",
    "                        \"sequence\": seq_move[read_id][\"sequence\"],\n",
    "                        \"signal\": signal[read_id][\"signal\"],\n",
    "                        \"mv_table\": seq_move[read_id][\"mv_table\"],\n",
    "                        \"num_trimmed\": seq_move[read_id][\"num_trimmed\"],\n",
    "                        \"to_norm_shift\": seq_move[read_id][\"shift\"],\n",
    "                        \"to_norm_scale\": seq_move[read_id][\"scale\"],\n",
    "                        \"stride\": seq_move[read_id][\"stride\"],\n",
    "                        \"to_pA_shift\": signal[read_id][\"shift\"],\n",
    "                        \"to_pA_scale\": signal[read_id][\"scale\"],\n",
    "                    }\n",
    "    return read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_neighbord_feature_dict(feature, base_num):\n",
    "    windows_size = base_num - 1 // 2\n",
    "    for read_id in feature.keys():\n",
    "        for i in range(len(feature[read_id])):\n",
    "            nbase = []\n",
    "            nstd = []\n",
    "            nmean = []\n",
    "            nsig = []\n",
    "            if i < windows_size:\n",
    "                if i != 0:\n",
    "                    for k in range(i):\n",
    "                        nbase = (\n",
    "                            nbase\n",
    "                            + list(feature[read_id][k][\"base\"])\n",
    "                            * feature[read_id][k][\"num\"]\n",
    "                        )\n",
    "                        nstd = (\n",
    "                            nstd\n",
    "                            + list(feature[read_id][k][\"std\"])\n",
    "                            * feature[read_id][k][\"num\"]\n",
    "                        )\n",
    "                        nmean = (\n",
    "                            nmean\n",
    "                            + list(feature[read_id][k][\"mean\"])\n",
    "                            * feature[read_id][k][\"num\"]\n",
    "                        )\n",
    "                        nsig = nsig + feature[read_id][k][\"signal\"]\n",
    "                nbase = (\n",
    "                    nbase\n",
    "                    + list(feature[read_id][i][\"base\"])\n",
    "                    * (windows_size - i)\n",
    "                    * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nbase = (\n",
    "                    nbase\n",
    "                    + list(feature[read_id][i][\"base\"]) * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nstd = (\n",
    "                    nstd\n",
    "                    + list(feature[read_id][i][\"std\"])\n",
    "                    * (windows_size - i)\n",
    "                    * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nstd = (\n",
    "                    nstd + list(feature[read_id][i][\"std\"]) * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nmean = (\n",
    "                    nmean\n",
    "                    + list(feature[read_id][i][\"mean\"])\n",
    "                    * (windows_size - i)\n",
    "                    * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nmean = (\n",
    "                    nmean\n",
    "                    + list(feature[read_id][i][\"mean\"]) * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                for k in range(i, i + windows_size):\n",
    "                    nbase = (\n",
    "                        nbase\n",
    "                        + list(feature[read_id][k][\"base\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                    nstd = (\n",
    "                        nbase\n",
    "                        + list(feature[read_id][k][\"std\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                    nmean = (\n",
    "                        nbase\n",
    "                        + list(feature[read_id][k][\"mean\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                nsig = nsig + feature[read_id][i][\"signal\"] * (windows_size - i)\n",
    "                nsig = nsig + feature[read_id][i][\"signal\"]\n",
    "                for k in range(i, i + windows_size):\n",
    "                    nsig = nsig + feature[read_id][k][\"signal\"]\n",
    "            elif (len(feature[read_id]) - 1) - i < windows_size:\n",
    "                for k in range(i - windows_size, i):\n",
    "                    nbase = (\n",
    "                        nbase\n",
    "                        + list(feature[read_id][k][\"base\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                    nstd = (\n",
    "                        nstd\n",
    "                        + list(feature[read_id][k][\"std\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                    nmean = (\n",
    "                        nmean\n",
    "                        + list(feature[read_id][k][\"mean\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                nbase = (\n",
    "                    nbase\n",
    "                    + list(feature[read_id][i][\"base\"]) * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nstd = (\n",
    "                    nstd + list(feature[read_id][i][\"std\"]) * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nmean = (\n",
    "                    nmean\n",
    "                    + list(feature[read_id][i][\"mean\"]) * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                for k in range(i - windows_size, i):\n",
    "                    nsig = nsig + feature[read_id][k][\"signal\"]\n",
    "                nsig = nsig + feature[read_id][i][\"signal\"]\n",
    "                if i != len(feature[read_id]) - 1:\n",
    "                    for k in range(i, len(feature[read_id]) - 1):\n",
    "                        nbase = (\n",
    "                            nbase\n",
    "                            + list(feature[read_id][k][\"base\"])\n",
    "                            * feature[read_id][k][\"num\"]\n",
    "                        )\n",
    "                        nstd = (\n",
    "                            nstd\n",
    "                            + list(feature[read_id][k][\"std\"])\n",
    "                            * feature[read_id][k][\"num\"]\n",
    "                        )\n",
    "                        nmean = (\n",
    "                            nmean\n",
    "                            + list(feature[read_id][k][\"mean\"])\n",
    "                            * feature[read_id][k][\"num\"]\n",
    "                        )\n",
    "                        nsig = nsig + feature[read_id][k][\"signal\"]\n",
    "                nbase = (\n",
    "                    nbase\n",
    "                    + list(feature[read_id][i][\"base\"])\n",
    "                    * (windows_size - ((len(feature[read_id]) - 1) - i))\n",
    "                    * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nstd = (\n",
    "                    nstd\n",
    "                    + list(feature[read_id][i][\"std\"])\n",
    "                    * (windows_size - ((len(feature[read_id]) - 1) - i))\n",
    "                    * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nmean = (\n",
    "                    nmean\n",
    "                    + list(feature[read_id][i][\"mean\"])\n",
    "                    * (windows_size - ((len(feature[read_id]) - 1) - i))\n",
    "                    * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nsig = nsig + feature[read_id][i][\"signal\"] * (\n",
    "                    windows_size - ((len(feature[read_id]) - 1) - i)\n",
    "                )\n",
    "            else:\n",
    "                for k in range(i - windows_size, i):\n",
    "                    nbase = (\n",
    "                        nbase\n",
    "                        + list(feature[read_id][k][\"base\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                    nstd = (\n",
    "                        nstd\n",
    "                        + list(feature[read_id][k][\"std\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                    nmean = (\n",
    "                        nmean\n",
    "                        + list(feature[read_id][k][\"mean\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                    nsig = nsig + feature[read_id][k][\"signal\"]\n",
    "                nbase = (\n",
    "                    nbase\n",
    "                    + list(feature[read_id][i][\"base\"]) * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nstd = (\n",
    "                    nstd + list(feature[read_id][i][\"std\"]) * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nmean = (\n",
    "                    nmean\n",
    "                    + list(feature[read_id][i][\"mean\"]) * feature[read_id][i][\"num\"]\n",
    "                )\n",
    "                nsig = nsig + feature[read_id][i][\"signal\"]\n",
    "                for k in range(i, i + windows_size):\n",
    "                    nbase = (\n",
    "                        nbase\n",
    "                        + list(feature[read_id][k][\"base\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                    nstd = (\n",
    "                        nstd\n",
    "                        + list(feature[read_id][k][\"std\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                    nmean = (\n",
    "                        nmean\n",
    "                        + list(feature[read_id][k][\"mean\"]) * feature[read_id][k][\"num\"]\n",
    "                    )\n",
    "                    nsig = nsig + feature[read_id][k][\"signal\"]\n",
    "            feature[read_id][i].update(\n",
    "                {\"nbase\": nbase, \"nsig\": nsig, \"nstd\": nstd, \"nmean\": nmean}\n",
    "            )\n",
    "\n",
    "\n",
    "def caculate_batch_feature_for_each_base_dict(read_q, feature_q, base_num=0):\n",
    "    feature = {}\n",
    "    print(\"extrac_features process-{} starts\".format(os.getpid()))\n",
    "    read_num = 0\n",
    "    while True:\n",
    "        if read_q.empty():\n",
    "            time.sleep(10)\n",
    "        read_batch = read_q.get()\n",
    "        if read_batch == \"kill\":\n",
    "            read_q.put(\"kill\")\n",
    "            break\n",
    "        read_num += len(read_batch)\n",
    "        for read_id in read_batch.keys():\n",
    "            feature[read_id] = []\n",
    "            sequence = read_batch[read_id][\"sequence\"]\n",
    "            movetable = read_batch[read_id][\"mv_table\"]\n",
    "            stride = read_batch[read_id][\"stride\"]\n",
    "            # num_trimmed = read[read_id]['num_trimmed']\n",
    "            trimed_signals = norm_signal_read_id(read_batch[read_id])  # 筛掉背景信号,norm\n",
    "            move_pos = np.append(np.argwhere(movetable == 1).flatten(), len(movetable))\n",
    "            # print(len(move_pos))\n",
    "            for move_idx in range(len(move_pos) - 1):\n",
    "                start, end = move_pos[move_idx], move_pos[move_idx + 1]\n",
    "                signal = trimed_signals[(start * stride) : (end * stride)].tolist()\n",
    "                mean = np.mean(signal)\n",
    "                std = np.std(signal)\n",
    "                num = end - start\n",
    "                # print(move_idx)\n",
    "                feature[read_id].append(\n",
    "                    {\n",
    "                        \"signal\": signal,\n",
    "                        \"std\": str(std),\n",
    "                        \"mean\": str(mean),\n",
    "                        \"num\": int(num * stride),\n",
    "                        \"base\": sequence[move_idx],\n",
    "                    }\n",
    "                )\n",
    "        # feature_q.append(feature)\n",
    "        if base_num != 0:\n",
    "            _get_neighbord_feature(feature, base_num)\n",
    "        feature_q.put(feature)\n",
    "        print(\n",
    "            \"extrac_features process-{} ending, proceed {} read batch\".format(\n",
    "                os.getpid(), read_num\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def _prepare_read_dict(read_q, read, batch_size=1000):\n",
    "    i = 0\n",
    "    read_batch = {}\n",
    "    for read_id in read.keys():\n",
    "        # print(read_id)\n",
    "        read_batch[read_id] = {}\n",
    "        read_batch[read_id].update(read[read_id])\n",
    "        i = i + 1\n",
    "        if i == batch_size:\n",
    "            i = 0\n",
    "            read_q.put(read_batch)\n",
    "            read_batch = {}\n",
    "    read_q.put(read_batch)\n",
    "\n",
    "\n",
    "def write_feature_dict(feature_q):\n",
    "    print(\"write_process-{} starts\".format(os.getpid()))\n",
    "    with open(\"/homeb/xiaoyf/data/HG002/example/feature.txt\", \"w\") as f:\n",
    "        while True:\n",
    "            if feature_q.empty():\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            feature = feature_q.get()\n",
    "            if feature == \"kill\":\n",
    "                print(\"write_process-{} finished\".format(os.getpid()))\n",
    "                break\n",
    "            for read_id in feature.keys():\n",
    "                # f.write(read_id+'\\t')\n",
    "                for i in range(len(feature[read_id])):\n",
    "                    f.write(\n",
    "                        str(feature[read_id][i][\"nbase\"])\n",
    "                        + \"\\t\"\n",
    "                        + str(feature[read_id][i][\"nsig\"])\n",
    "                        + \"\\t\"\n",
    "                        + str(feature[read_id][i][\"nstd\"])\n",
    "                        + \"\\t\"\n",
    "                        + str(feature[read_id][i][\"nmean\"])\n",
    "                        + \"\\n\"\n",
    "                    )\n",
    "            f.flush()\n",
    "\n",
    "\n",
    "def extract_feature():\n",
    "    start = time.time()\n",
    "    feature_q = Queue()\n",
    "    read_q = Queue()\n",
    "    _prepare_read(read_q, read, batch_size=500)\n",
    "    feature_procs = []\n",
    "    nproc = 4\n",
    "    read_q.put(\"kill\")\n",
    "    for _ in range(nproc):\n",
    "        p = mp.Process(\n",
    "            target=caculate_batch_feature_for_each_base,\n",
    "            args=(\n",
    "                read_q,\n",
    "                feature_q,\n",
    "                21,\n",
    "            ),\n",
    "        )\n",
    "        p.daemon = True\n",
    "        p.start()\n",
    "        feature_procs.append(p)\n",
    "\n",
    "    p_w = mp.Process(target=write_feature, args=(feature_q,))\n",
    "    p_w.daemon = True\n",
    "    p_w.start()\n",
    "\n",
    "    for p in feature_procs:\n",
    "        p.join()\n",
    "\n",
    "    feature_q.put(\"kill\")\n",
    "    p_w.join()\n",
    "    print(\"[main]extract_features costs %.1f seconds..\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0:read_id,1:signal,2:std,3:mean,4:num,5:base\n",
    "def _get_neighbord_feature(sequence, feature, base_num):\n",
    "    # 数据预处理主要速度瓶颈，同样的reads数，不运行这个函数大概快了十倍，从二十多分钟减到两分钟\n",
    "    motif = \"CG\"\n",
    "    motif_seqs = get_motif_seqs(motif)\n",
    "    tsite_locs = get_refloc_of_methysite_in_motif(sequence, set(motif_seqs))\n",
    "    nfeature = []\n",
    "    windows_size = base_num - 1 // 2\n",
    "    for i in range(len(feature)):\n",
    "        nbase = []\n",
    "        nstd = []\n",
    "        nmean = []\n",
    "        nsig = []\n",
    "        if i not in tsite_locs:\n",
    "            continue\n",
    "        if i < windows_size:\n",
    "            if i != 0:\n",
    "                for k in range(i):\n",
    "                    nbase = nbase + list(feature[k][5]) * signal_sample\n",
    "                    nstd = nstd + list(feature[k][2]) * signal_sample\n",
    "                    nmean = nmean + list(feature[k][3]) * signal_sample\n",
    "                    nsig = nsig + feature[k][1]\n",
    "            nbase = nbase + list(feature[i][5]) * (windows_size - i) * signal_sample\n",
    "            nbase = nbase + list(feature[i][5]) * signal_sample\n",
    "            nstd = nstd + list(feature[i][2]) * (windows_size - i) * signal_sample\n",
    "            nstd = nstd + list(feature[i][2]) * signal_sample\n",
    "            nmean = nmean + list(feature[i][3]) * (windows_size - i) * signal_sample\n",
    "            nmean = nmean + list(feature[i][3]) * signal_sample\n",
    "            nsig = nsig + feature[i][1] * (windows_size - i)\n",
    "            nsig = nsig + feature[i][1]\n",
    "            for k in range(i, i + windows_size):\n",
    "                nbase = nbase + list(feature[k][5]) * signal_sample\n",
    "                nstd = nbase + list(feature[k][2]) * signal_sample\n",
    "                nmean = nbase + list(feature[k][3]) * signal_sample\n",
    "                nsig = nsig + feature[k][1]\n",
    "        elif (len(feature[i]) - 1) - i < windows_size:\n",
    "            for k in range(i - windows_size, i):\n",
    "                nbase = nbase + list(feature[k][5]) * signal_sample\n",
    "                nstd = nstd + list(feature[k][2]) * signal_sample\n",
    "                nmean = nmean + list(feature[k][3]) * signal_sample\n",
    "                nsig = nsig + feature[k][1]\n",
    "            nbase = nbase + list(feature[i][5]) * signal_sample\n",
    "            nstd = nstd + list(feature[i][2]) * signal_sample\n",
    "            nmean = nmean + list(feature[i][3]) * signal_sample\n",
    "            nsig = nsig + feature[i][1]\n",
    "            if i != len(feature[i]) - 1:\n",
    "                for k in range(i, len(feature[i]) - 1):\n",
    "                    nbase = nbase + list(feature[k][5]) * signal_sample\n",
    "                    nstd = nstd + list(feature[k][2]) * signal_sample\n",
    "                    nmean = nmean + list(feature[k][3]) * signal_sample\n",
    "                    nsig = nsig + feature[k][1]\n",
    "            nbase = (\n",
    "                nbase\n",
    "                + list(feature[i][5])\n",
    "                * (windows_size - ((len(feature[i]) - 1) - i))\n",
    "                * signal_sample\n",
    "            )\n",
    "            nstd = (\n",
    "                nstd\n",
    "                + list(feature[i][2])\n",
    "                * (windows_size - ((len(feature[i]) - 1) - i))\n",
    "                * signal_sample\n",
    "            )\n",
    "            nmean = (\n",
    "                nmean\n",
    "                + list(feature[i][3])\n",
    "                * (windows_size - ((len(feature[i]) - 1) - i))\n",
    "                * signal_sample\n",
    "            )\n",
    "            nsig = nsig + feature[i][1] * (windows_size - ((len(feature[i]) - 1) - i))\n",
    "        else:\n",
    "            for k in range(i - windows_size, i):\n",
    "                nbase = nbase + list(feature[k][5]) * signal_sample\n",
    "                nstd = nstd + list(feature[k][2]) * signal_sample\n",
    "                nmean = nmean + list(feature[k][3]) * signal_sample\n",
    "                nsig = nsig + feature[k][1]\n",
    "            nbase = nbase + list(feature[i][5]) * signal_sample\n",
    "            nstd = nstd + list(feature[i][2]) * signal_sample\n",
    "            nmean = nmean + list(feature[i][3]) * signal_sample\n",
    "            nsig = nsig + feature[i][1]\n",
    "            for k in range(i, i + windows_size):\n",
    "                nbase = nbase + list(feature[k][5]) * signal_sample\n",
    "                nstd = nstd + list(feature[k][2]) * signal_sample\n",
    "                nmean = nmean + list(feature[k][3]) * signal_sample\n",
    "                nsig = nsig + feature[k][1]\n",
    "        # feature[read_id][i].update({'nbase':nbase,'nsig':nsig,'nstd':nstd,'nmean':nmean})\n",
    "        nfeature.append([feature[i][0], nbase, nsig])\n",
    "\n",
    "        # 0:read_id,1:nbase,2:nsig,3:nstd,4:nmean\n",
    "        # logger.debug('feature id: {}, feature:{}'.format(str(feature[0]),(str(nbase),str(nsig),str(nstd),str(nmean))))\n",
    "    return nfeature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsignal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
