{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pod5 as p5\n",
    "import pysam\n",
    "from multiprocessing import Queue#,Manager,Pool\n",
    "import time\n",
    "import os\n",
    "import signal\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from utils.utils import parse_args\n",
    "from utils.log import get_logger,init_logger\n",
    "\n",
    "signal.signal(signal.SIGPIPE, signal.SIG_IGN)  # 忽略SIGPIPE信号\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def extract_signal_from_pod5(pod5_path)-> list:\n",
    "    signals=[]\n",
    "    with p5.Reader(pod5_path) as reader:\n",
    "        for read_record in reader.reads():\n",
    "            #signals[str(read_record.read_id)] = {'signal':read_record.signal,'shift':read_record.calibration.offset,'scale':read_record.calibration.scale}#不加str会变成UUID，很奇怪\n",
    "            signals.append([str(read_record.read_id),read_record.signal,read_record.calibration.offset,read_record.calibration.scale])\n",
    "            #0:read_id,1:signal,2:shift,3:scale\n",
    "    return signals\n",
    "def extract_move_from_bam(bam_path)-> list:\n",
    "    seq_move=[]\n",
    "    bamfile = pysam.AlignmentFile(bam_path, \"rb\",check_sq=False)\n",
    "    try:\n",
    "        for read in bamfile.fetch(until_eof=True):#暂时不使用索引，使用返回是空值\n",
    "            #print(read.query_name)\n",
    "            tags=dict(read.tags)\n",
    "            mv_tag=tags['mv']\n",
    "            ts_tag=tags['ts']\n",
    "            sm_tag=tags[\"sm\"]\n",
    "            sd_tag=tags[\"sd\"]\n",
    "            #read.update({read.query_name:{\"sequence\":read.query_sequence,\"stride\":mv_tag[0],\"mv_table\":np.array(mv_tag[1:]),\"num_trimmed\":ts_tag,\"shift\":sm_tag,\"scale\":sd_tag}})\n",
    "            seq_move.append([read.query_name,read.query_sequence,mv_tag[0],np.array(mv_tag[1:]),ts_tag,sm_tag,sd_tag])\n",
    "    except ValueError:\n",
    "        print('bam don\\'t has index')\n",
    "        for read in bamfile.fetch(until_eof=True,multiple_iterators=False):\n",
    "            tags=dict(read.tags)\n",
    "            mv_tag=tags['mv']\n",
    "            ts_tag=tags['ts']\n",
    "            sm_tag=tags[\"sm\"]\n",
    "            sd_tag=tags[\"sd\"]\n",
    "            seq_move.append([read.query_name,read.query_sequence,mv_tag[0],np.array(mv_tag[1:]),ts_tag,sm_tag,sd_tag])\n",
    "            #0:read_id,1:sequence,2:stride,3:mv_table,4:num_trimmed,5:to_norm_shift,6:to_norm_scale\n",
    "            #read[read.query_name] = {\"sequence\":read.query_sequence,\"stride\":mv_tag[0],\"mv_table\":np.array(mv_tag[1:]),\"num_trimmed\":ts_tag,\"shift\":sm_tag,\"scale\":sd_tag}\n",
    "    return seq_move\n",
    "def read_from_pod5_bam(pod5_path,bam_path,read_id=None)-> list:\n",
    "    read=[]\n",
    "    signal = extract_signal_from_pod5(pod5_path)\n",
    "    seq_move = extract_move_from_bam(bam_path)\n",
    "    if read_id is not None:\n",
    "        for i in range(len(seq_move)):\n",
    "            if seq_move[i][0]==read_id:\n",
    "                if seq_move[i][1] is not None:\n",
    "                    for j in range(len(signal)):\n",
    "                        if signal[j][0]==seq_move[i][0]:\n",
    "                            read.append([signal[j][0],signal[j][1],signal[j][2],signal[j][3],\n",
    "                            seq_move[i][1],seq_move[i][2],seq_move[i][3],seq_move[i][4],seq_move[i][5],seq_move[i][6]])\n",
    "        \n",
    "    else:\n",
    "        for i in range(len(seq_move)):\n",
    "            if seq_move[i][1] is not None:\n",
    "                for j in range(len(signal)):\n",
    "                    if signal[j][0]==seq_move[i][0]:\n",
    "                        read.append([signal[j][0],signal[j][1],signal[j][2],signal[j][3],\n",
    "                        seq_move[i][1],seq_move[i][2],seq_move[i][3],seq_move[i][4],seq_move[i][5],seq_move[i][6]])\n",
    "                #0:read_id,1:signal,2:to_pA_shift,3:to_pA_scale,4:sequence,5:stride,6:mv_table,7:num_trimmed,8:to_norm_shift,9:to_norm_scale\n",
    "                    \n",
    "                \n",
    "    return read\n",
    "\n",
    "#0:read_id,1:signal,2:std,3:mean,4:num,5:base \n",
    "def _get_neighbord_feature(feature,base_num):\n",
    "    #数据预处理主要速度瓶颈，同样的reads数，不运行这个函数大概快了十倍，从二十多分钟减到两分钟\n",
    "    nfeature=[]\n",
    "    windows_size=base_num-1//2\n",
    "    for i in range(len(feature)):\n",
    "        nbase=[]\n",
    "        nstd=[]\n",
    "        nmean=[]\n",
    "        nsig=[]\n",
    "        if i<windows_size:                   \n",
    "            if i!=0:\n",
    "                for k in range(i):\n",
    "                    nbase=nbase+list(feature[k][5])*feature[k][4]\n",
    "                    nstd=nstd+list(feature[k][2])*feature[k][4]\n",
    "                    nmean=nmean+list(feature[k][3])*feature[k][4]\n",
    "                    nsig=nsig+feature[k][1]\n",
    "            nbase=nbase+list(feature[i][5])*(windows_size-i)*feature[i][4]\n",
    "            nbase=nbase+list(feature[i][5])*feature[i][4]\n",
    "            nstd=nstd+list(feature[i][2])*(windows_size-i)*feature[i][4]\n",
    "            nstd=nstd+list(feature[i][2])*feature[i][4]\n",
    "            nmean=nmean+list(feature[i][3])*(windows_size-i)*feature[i][4]\n",
    "            nmean=nmean+list(feature[i][3])*feature[i][4]\n",
    "            nsig=nsig+feature[i][1]*(windows_size-i)\n",
    "            nsig=nsig+feature[i][1]\n",
    "            for k in range(i,i+windows_size):\n",
    "                nbase=nbase+list(feature[k][5])*feature[k][4]\n",
    "                nstd=nbase+list(feature[k][2])*feature[k][4]\n",
    "                nmean=nbase+list(feature[k][3])*feature[k][4]\n",
    "                nsig=nsig+feature[k][1]\n",
    "        elif (len(feature[i])-1)-i<windows_size:\n",
    "            for k in range(i-windows_size,i):\n",
    "                nbase=nbase+list(feature[k][5])*feature[k][4]\n",
    "                nstd=nstd+list(feature[k][2])*feature[k][4]\n",
    "                nmean=nmean+list(feature[k][3])*feature[k][4]\n",
    "                nsig=nsig+feature[k][1]\n",
    "            nbase=nbase+list(feature[i][5])*feature[i][4]\n",
    "            nstd=nstd+list(feature[i][2])*feature[i][4]\n",
    "            nmean=nmean+list(feature[i][3])*feature[i][4]        \n",
    "            nsig=nsig+feature[i][1]                   \n",
    "            if i!=len(feature[i])-1:\n",
    "                for k in range(i,len(feature[i])-1):\n",
    "                    nbase=nbase+list(feature[k][5])*feature[k][4]\n",
    "                    nstd=nstd+list(feature[k][2])*feature[k][4]\n",
    "                    nmean=nmean+list(feature[k][3])*feature[k][4]\n",
    "                    nsig=nsig+feature[k][1]\n",
    "            nbase=nbase+list(feature[i][5])*(windows_size-((len(feature[i])-1)-i))*feature[i][4]\n",
    "            nstd=nstd+list(feature[i][2])*(windows_size-((len(feature[i])-1)-i))*feature[i][4]\n",
    "            nmean=nmean+list(feature[i][3])*(windows_size-((len(feature[i])-1)-i))*feature[i][4]\n",
    "            nsig=nsig+feature[i][1]*(windows_size-((len(feature[i])-1)-i))\n",
    "        else:\n",
    "            for k in range(i-windows_size,i):\n",
    "                nbase=nbase+list(feature[k][5])*feature[k][4]\n",
    "                nstd=nstd+list(feature[k][2])*feature[k][4]\n",
    "                nmean=nmean+list(feature[k][3])*feature[k][4]\n",
    "                nsig=nsig+feature[k][1]\n",
    "            nbase=nbase+list(feature[i][5])*feature[i][4]\n",
    "            nstd=nstd+list(feature[i][2])*feature[i][4]\n",
    "            nmean=nmean+list(feature[i][3])*feature[i][4]\n",
    "            nsig=nsig+feature[i][1]\n",
    "            for k in range(i,i+windows_size):\n",
    "                nbase=nbase+list(feature[k][5])*feature[k][4]\n",
    "                nstd=nstd+list(feature[k][2])*feature[k][4]\n",
    "                nmean=nmean+list(feature[k][3])*feature[k][4]\n",
    "                nsig=nsig+feature[k][1]\n",
    "        #feature[read_id][i].update({'nbase':nbase,'nsig':nsig,'nstd':nstd,'nmean':nmean})\n",
    "        nfeature.append([feature[i][0],nbase,nsig,nstd,nmean])\n",
    "        \n",
    "        #0:read_id,1:nbase,2:nsig,3:nstd,4:nmean\n",
    "        #LOGGER.debug('feature id: {}, feature:{}'.format(str(feature[0]),(str(nbase),str(nsig),str(nstd),str(nmean))))\n",
    "    return nfeature\n",
    "        \n",
    "#0:read_id,1:signal,2:to_pA_shift,3:to_pA_scale,4:sequence,5:stride,6:mv_table,7:num_trimmed,8:to_norm_shift,9:to_norm_scale\n",
    "def norm_signal_read_id(signal):\n",
    "    shift_scale_norm=[]\n",
    "    signal_norm=[]\n",
    "    shift_scale_norm=[(signal[8]/signal[3])-signal[2],(signal[9]/signal[3])]\n",
    "    #0:shift,1:scale\n",
    "\n",
    "    num_trimmed=signal[7]\n",
    "    #print('num_trimmed:{} and signal:{}'.format(num_trimmed,signal[1]))\n",
    "    #print('shift:{} and scale:{}'.format(shift_scale_norm[0],shift_scale_norm[1]))\n",
    "    signal_norm=(signal[1][num_trimmed:] - shift_scale_norm[0]) / shift_scale_norm[1]        \n",
    "    return signal_norm\n",
    "\n",
    "def caculate_batch_feature_for_each_base(read_batch):\n",
    "    print(\"extrac_features process-{} starts\".format(os.getpid()))\n",
    "    LOGGER.info(\"extrac_features process-{} starts\".format(os.getpid()))\n",
    "    read_num = 0\n",
    "    base_num = 21\n",
    "    for read_one in read_batch:\n",
    "        feature=[]\n",
    "        #print(read_one)            \n",
    "        sequence = read_one[4]\n",
    "        stride = read_one[5]\n",
    "        movetable = read_one[6]           \n",
    "        #num_trimmed = read[read_id]['num_trimmed']\n",
    "        trimed_signals = norm_signal_read_id(read_one)#筛掉背景信号,norm\n",
    "        move_pos = np.append(np.argwhere(movetable == 1).flatten(), len(movetable))\n",
    "        #print(len(move_pos))\n",
    "        for move_idx in range(len(move_pos) - 1):\n",
    "            start, end = move_pos[move_idx], move_pos[move_idx + 1]\n",
    "            signal=trimed_signals[(start * stride):(end * stride)].tolist()\n",
    "            mean=np.mean(signal)\n",
    "            std=np.std(signal)\n",
    "            num=end-start\n",
    "            #print(move_idx)\n",
    "            feature.append([read_one[0],signal,str(std),str(mean),int(num*stride),sequence[move_idx]])\n",
    "            #0:read_id,1:signal,2:std,3:mean,4:num,5:base        \n",
    "            #feature[read_id].append({'signal':signal,'std':str(std),'mean':str(mean),'num':int(num*stride),'base':sequence[move_idx]})\n",
    "        if base_num!=0:\n",
    "            nfeature=_get_neighbord_feature(feature,base_num)\n",
    "            LOGGER.debug(\"extract neigbor features for read_id:{}\".format(read_one[0]))\n",
    "            yield nfeature\n",
    "    LOGGER.info(\"extrac_features process-{} ending, proceed {} read batch\".format(os.getpid(), read_num))     \n",
    "\n",
    "\n",
    "def _prepare_read(read,batch_size=1000):\n",
    "    i=0\n",
    "    #j=0\n",
    "    read_batch=[]\n",
    "    for read_one in read:\n",
    "        read_batch.append(read_one)\n",
    "        i=i+1\n",
    "        #j=j+1\n",
    "        #if j==40:\n",
    "        #    break\n",
    "        if i==batch_size:\n",
    "            i=0\n",
    "            yield read_batch\n",
    "            read_batch=[]\n",
    "    LOGGER.info('total batch number is {}'.format((len(read)-1)//batch_size+1))\n",
    "    yield read_batch\n",
    "\n",
    "\n",
    "def write_feature(feature_batch):\n",
    "    #print(\"write_process-{} starts\".format(os.getpid()))\n",
    "    LOGGER.info(\"write_process-{} starts\".format(os.getpid()))\n",
    "    dataset=[]\n",
    "    #pos=bar_q.get()\n",
    "    #write_feature_bar = tqdm(total = read_number, desc='write_feature', position=pos,colour='green')\n",
    "    #bar_q.put(pos+1)\n",
    "    try:\n",
    "        LOGGER.info('write process get bases number:{}'.format(len(feature_batch)))\n",
    "        for feature in feature_batch:\n",
    "                \n",
    "                dataset.append(feature) \n",
    "        np_data = np.array(dataset)\n",
    "        np.save(\"/home/xiaoyf/methylation/deepsignal/log/data.npy\", np_data)\n",
    "       \n",
    "    except Exception as e:\n",
    "        LOGGER.error('error in writing features')\n",
    "        print(e)\n",
    "    #finally:\n",
    "        \n",
    "    #write_pbar.close()\n",
    "            \n",
    "def bar_listener(p_bar,desc='',position=1,number=4000):\n",
    "    bar = tqdm(total = number, desc=desc, position=position)\n",
    "    for item in iter(p_bar.get, None):\n",
    "        bar.update(item)\n",
    "\n",
    "def extract_feature(read,output_file,nproc = 4,batch_size=20):\n",
    "    start = time.time()\n",
    "\n",
    "    read_number=len(read)\n",
    "\n",
    "    write_filename=output_file\n",
    "    a=[]\n",
    "    with Pool(nproc) as p:\n",
    "        a.append(p.imap(caculate_batch_feature_for_each_base, _prepare_read(read,batch_size)))\n",
    "        #tqdm(p.imap(write_feature,tqdm(caculate_batch_feature_for_each_base(_prepare_read(read,batch_size)) , total=read_number, desc='extract_features'))\n",
    "        #     , total=read_number, desc='write_features')\n",
    "\n",
    "    LOGGER.info(\"[main]extract_features costs %.1f seconds..\" %(time.time() - start))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E::idx_find_and_load] Could not retrieve index file for '/homeb/xiaoyf/data/HG002/example/bam/has_moves.bam'\n",
      "0it [00:09, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--log-file', type=str, default=\"/home/xiaoyf/methylation/deepsignal/log/test.log\",\n",
    "                        help='log store address.')\n",
    "    parser.add_argument('--output-file', type=str, default=\"/home/xiaoyf/methylation/deepsignal/log/data.npy\",\n",
    "                        help='feature file store address.')\n",
    "    parser.add_argument('--pod5-file', type=str, default='/homeb/xiaoyf/data/HG002/example/pod5/output.pod5',\n",
    "                        help='pod5 file store address.')\n",
    "    parser.add_argument('--bam-file', type=str, default='/homeb/xiaoyf/data/HG002/example/bam/has_moves.bam',\n",
    "                        help='bam file store address.')\n",
    "    parser.add_argument('--nproc', type=int, default=4,\n",
    "                        help='number of processes in extract features.')\n",
    "    parser.add_argument('--batch-size', type=int, default=200,\n",
    "                        help='size of batch in extract features.')\n",
    "    parser.add_argument('--window-size', type=int, default=21,\n",
    "                        help='size of window in extract neighbor features for target base.')\n",
    "    args = parser.parse_args([])\n",
    "    init_logger(args.log_file)\n",
    "    batch_size = args.batch_size\n",
    "    window_size = args.window_size\n",
    "    output_file = args.output_file\n",
    "    log_file = args.log_file\n",
    "    pod5_path = args.pod5_file\n",
    "    bam_path = args.bam_file\n",
    "    nproc = args.nproc\n",
    "    \n",
    "    read=read_from_pod5_bam(pod5_path,bam_path)\n",
    "    start = time.time()\n",
    "\n",
    "    read_number=len(read)\n",
    "\n",
    "    write_filename=output_file\n",
    "    a=[]\n",
    "    with Pool(nproc) as p:\n",
    "        a.append(tqdm(p.imap(caculate_batch_feature_for_each_base, _prepare_read(read,batch_size))))\n",
    "        #tqdm(p.imap(write_feature,tqdm(caculate_batch_feature_for_each_base(_prepare_read(read,batch_size)) , total=read_number, desc='extract_features'))\n",
    "        #     , total=read_number, desc='write_features')\n",
    "\n",
    "    LOGGER.info(\"[main]extract_features costs %.1f seconds..\" %(time.time() - start))\n",
    "    #extract_feature(read,output_file,nproc,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 1156857.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.017298s\n",
      "Sum of squares: 333383335000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def square(numbers):\n",
    "    for n in numbers:\n",
    "        yield n * n\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    numbers = range(1, 10001)\n",
    "    start_time = time.time()\n",
    "    squared_numbers = list(tqdm(square(numbers), total=len(numbers)))\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken: {end_time - start_time:.6f}s\")\n",
    "    print(f\"Sum of squares: {sum(squared_numbers)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsignal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
