{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch.autograd as autograd\n",
    "#import sys\n",
    "#sys.path.append('/home/xiaoyf/methylation/deepsignal/')\n",
    "#from deepsignal3.utils import constants\n",
    "use_cuda=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/homeb/xiaoyf/data/HG002/R9.4/samples_CG.hc_poses.r30m.tsv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgzip\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcsv\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mwith\u001b[39;00m gzip\u001b[39m.\u001b[39;49mopen(\u001b[39m'\u001b[39;49m\u001b[39m/homeb/xiaoyf/data/HG002/R9.4/samples_CG.hc_poses.r30m.tsv.gz\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39mdecode()\n\u001b[1;32m      5\u001b[0m     tsv_reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(data, delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepsignal/lib/python3.8/gzip.py:58\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m gz_mode \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filename, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m, os\u001b[39m.\u001b[39mPathLike)):\n\u001b[0;32m---> 58\u001b[0m     binary_file \u001b[39m=\u001b[39m GzipFile(filename, gz_mode, compresslevel)\n\u001b[1;32m     59\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mhasattr\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mwrite\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     60\u001b[0m     binary_file \u001b[39m=\u001b[39m GzipFile(\u001b[39mNone\u001b[39;00m, gz_mode, compresslevel, filename)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepsignal/lib/python3.8/gzip.py:173\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m     mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m fileobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     fileobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmyfileobj \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, mode \u001b[39mor\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m filename \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fileobj, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/homeb/xiaoyf/data/HG002/R9.4/samples_CG.hc_poses.r30m.tsv.gz'"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import csv\n",
    "with gzip.open('/homeb/xiaoyf/data/HG002/R9.4/samples_CG.hc_poses.r30m.tsv.gz', 'rt') as f:\n",
    "    data = f.read().decode()\n",
    "    tsv_reader = csv.reader(data, delimiter=\"\\t\")\n",
    "\n",
    "    number_of_lines = 10\n",
    "\n",
    "    for i in range(number_of_lines):\n",
    "        row = next(tsv_reader)\n",
    "        print(i, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39182/2179687221.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  chunks = pd.read_csv('/homeb/xiaoyf/data/HG002/R9.4/samples_CG.hc_poses.r30m.tsv',sep='\\t',error_bad_lines=False,iterator = True,header=None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "chunks = pd.read_csv('/homeb/xiaoyf/data/HG002/R9.4/samples_CG.hc_poses.r30m.tsv',sep='\\t',error_bad_lines=False,iterator = True,header=None)\n",
    "chunk = chunks.get_chunk(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在deepsignal2里面一个碱基固定对应16个信号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 16)\n",
      "(17, 16)\n",
      "(17, 16)\n",
      "(17, 16)\n",
      "(17, 16)\n"
     ]
    }
   ],
   "source": [
    "sigs=chunk.iloc[:,-2]\n",
    "for s in sigs:\n",
    "    sig=np.array([[float(y) for y in x.split(\",\")] for x in s.split(\";\")])\n",
    "    print(sig.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--conv-in\", type=int, default=4, help=\"Input sequence features\"\n",
    "    )\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=512, required=False)\n",
    "    parser.add_argument(\"--step_interval\", type=int, default=100, required=False)\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001, required=False)\n",
    "    parser.add_argument(\n",
    "        \"--train-file\",\n",
    "        type=str,\n",
    "        help=\"feature file used in trainning\",\n",
    "        default=\"/homeb/xiaoyf/data/HG002/R9.4/samples_CG.hc_poses.r30m.tsv\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_dir\", type=str, default=\"/home/xiaoyf/methylation/deepsignal/log/\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_epoch_num\",\n",
    "        action=\"store\",\n",
    "        default=10,\n",
    "        type=int,\n",
    "        required=False,\n",
    "        help=\"max epoch num, default 10\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_epoch_num\",\n",
    "        action=\"store\",\n",
    "        default=5,\n",
    "        type=int,\n",
    "        required=False,\n",
    "        help=\"min epoch num, default 5\",\n",
    "    )\n",
    "    return parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(tensor, dim=-1):\n",
    "    squared_norm = (tensor**2).sum(dim=dim, keepdim=True)\n",
    "    scale = squared_norm / (1 + squared_norm)\n",
    "    return scale * tensor / torch.sqrt(squared_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squash(nn.Module):\n",
    "    def __init__(self, eps=10e-21, **kwargs):\n",
    "        super(Squash, self).__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, s):\n",
    "        n = torch.norm(s, dim=-1, keepdim=True)\n",
    "        return (1 - 1 / (torch.exp(n) + self.eps)) * (s / (n + self.eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_routing(x, iterations=3):\n",
    "    #x = x.unsqueeze(-1)\n",
    "    N = x.shape[1]  # num_caps\n",
    "    N1 = x.shape[2]  # in_caps\n",
    "    B = x.shape[0]\n",
    "    # feature_dim = x.shape[2]\n",
    "    #x:batch_size, num_caps, in_caps, out_channels\n",
    "    b = torch.zeros(B, N, N1,1).to(x.device)#batch_size, num_caps, in_caps\n",
    "    for _ in range(iterations):\n",
    "        #print('input x\\'s batch_size: {}, num_caps: {}, in_caps: {}, out_channels: {}'.format(x.shape[0], x.shape[1], x.shape[2], x.shape[3]))\n",
    "        c = F.softmax(b, dim=1)#Softmax along num_caps\n",
    "        #batch_size, num_caps,caps_dim\n",
    "        #print('softmax result\\'s batch_size: {}, num_caps: {}, in_caps: {}, softmax_result: {}'.format(c.shape[0], c.shape[1], c.shape[2], c.shape[3]))\n",
    "        a = c*x\n",
    "        #print('a\\'s batch_size: {}, num_caps: {}, in_caps: {}, out_channels: {}'.format(a.shape[0], a.shape[1], a.shape[2], a.shape[3]))\n",
    "        s = torch.sum(a, dim=2).squeeze(-1)#sum across in_caps\n",
    "        #print('s\\'s batch_size: {}, num_caps: {}, out_channels: {}'.format(s.shape[0], s.shape[1], s.shape[2]))\n",
    "        v = squash(s)# apply \"squashing\" non-linearity along out_channels\n",
    "        #print('v\\'s batch_size: {}, num_caps: {}, out_channels: {}'.format(v.shape[0], v.shape[1], v.shape[2]))\n",
    "        #print('x shape: {}'.format(x.shape))\n",
    "        y = torch.matmul(x,v.unsqueeze(-1))\n",
    "        #print('y shape: {}'.format(y.shape))\n",
    "        #print('b shape: {}'.format(b.shape))\n",
    "        b = b + y\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x's batch_size: 2, num_caps: 2, in_caps: 5, out_channels: 205\n",
      "softmax result's batch_size: 2, num_caps: 2, in_caps: 5, softmax_result: 1\n",
      "a's batch_size: 2, num_caps: 2, in_caps: 5, out_channels: 205\n",
      "s's batch_size: 2, num_caps: 2, out_channels: 205\n",
      "v's batch_size: 2, num_caps: 2, out_channels: 205\n",
      "x shape: torch.Size([2, 2, 5, 205])\n",
      "y shape: torch.Size([2, 2, 5, 1])\n",
      "b shape: torch.Size([2, 2, 5, 1])\n",
      "input x's batch_size: 2, num_caps: 2, in_caps: 5, out_channels: 205\n",
      "softmax result's batch_size: 2, num_caps: 2, in_caps: 5, softmax_result: 1\n",
      "a's batch_size: 2, num_caps: 2, in_caps: 5, out_channels: 205\n",
      "s's batch_size: 2, num_caps: 2, out_channels: 205\n",
      "v's batch_size: 2, num_caps: 2, out_channels: 205\n",
      "x shape: torch.Size([2, 2, 5, 205])\n",
      "y shape: torch.Size([2, 2, 5, 1])\n",
      "b shape: torch.Size([2, 2, 5, 1])\n",
      "input x's batch_size: 2, num_caps: 2, in_caps: 5, out_channels: 205\n",
      "softmax result's batch_size: 2, num_caps: 2, in_caps: 5, softmax_result: 1\n",
      "a's batch_size: 2, num_caps: 2, in_caps: 5, out_channels: 205\n",
      "s's batch_size: 2, num_caps: 2, out_channels: 205\n",
      "v's batch_size: 2, num_caps: 2, out_channels: 205\n",
      "x shape: torch.Size([2, 2, 5, 205])\n",
      "y shape: torch.Size([2, 2, 5, 1])\n",
      "b shape: torch.Size([2, 2, 5, 1])\n",
      "torch.Size([2, 2, 205])\n"
     ]
    }
   ],
   "source": [
    "#x:batch_size, num_caps, in_caps, out_channels\n",
    "input_tensor=torch.randn(2, 2, 5,205)\n",
    "print(dynamic_routing(input_tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the input and output channels\n",
    "in_channels = 2\n",
    "out_channels = 1\n",
    "\n",
    "# Define the kernel size and dilation\n",
    "kernel_size = 2\n",
    "\n",
    "# Define the 1D dilated convolution layers\n",
    "conv1d_list = nn.ModuleList()\n",
    "for dilation in range(1, 6):\n",
    "    padding = \"same\"\n",
    "    conv1d_list.append(\n",
    "        nn.Conv1d(\n",
    "            in_channels, out_channels, kernel_size, dilation=dilation, padding=padding\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = torch.randn(1, in_channels, 21 * 5)\n",
    "\n",
    "# Apply the 1D dilated convolutions to the input tensor\n",
    "output_tensor_list = []\n",
    "for conv1d in conv1d_list:\n",
    "    print(conv1d(input_tensor).shape)\n",
    "    output_tensor_list.append(conv1d(input_tensor))\n",
    "\n",
    "# Concatenate the output tensors along the channel dimension\n",
    "output_tensor = torch.cat(output_tensor_list, dim=1)\n",
    "\n",
    "print(output_tensor.shape)\n",
    "print(dynamic_routing(output_tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 1, 1])\n",
      "torch.Size([1, 1, 5, 105, 1])\n",
      "torch.Size([1, 1, 105])\n"
     ]
    }
   ],
   "source": [
    "c = F.softmax(torch.zeros(1, 1, 5, 1, 1), dim=1)\n",
    "# print(c)\n",
    "print(c.shape)\n",
    "x = torch.randn(1, 5, 21 * 5, 1)\n",
    "a = x.matmul(c)\n",
    "# print(a)\n",
    "print(a.shape)\n",
    "s = torch.sum(x.matmul(c), dim=2).squeeze(-1)\n",
    "# print(s)\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 105])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoyifu/anaconda3/envs/deepsignal/lib/python3.8/site-packages/torch/nn/modules/conv.py:309: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "class PrimaryCapsuleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Create a primary capsule layer with the methodology described in 'Efficient-CapsNet: Capsule Network with Self-Attention Routing'.\n",
    "    Properties of each capsule s_n are exatracted using a 1D depthwise convolution.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    kernel_size[w]: int\n",
    "        depthwise conv kernel dimension\n",
    "    conv_num: int\n",
    "        number of primary capsules\n",
    "    feature_dimension: int\n",
    "        primary capsules dimension (number of properties)\n",
    "    conv_stride: int\n",
    "        depthwise conv strides\n",
    "    Methods\n",
    "    -------\n",
    "    forward(inputs)\n",
    "        compute the primary capsule layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_in=2,\n",
    "        conv_out=4,\n",
    "        #feature_dimension=256,#272,  # 21 * 5,\n",
    "        kernel_size=2,\n",
    "        conv_num=5,\n",
    "        #sig_len=21,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.conv_out = feature_dimension // conv_num#(conv_num * base_num)\n",
    "        self.conv_out = conv_out\n",
    "        self.conv_num = conv_num\n",
    "        self.primary_capsule_layer = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    in_channels=conv_in,#输入信号通道，词向量维度\n",
    "                    out_channels=self.conv_out,\n",
    "                    kernel_size=kernel_size,#第二个维度是由in_channels来决定的，所以实际上卷积大小为kernel_size*in_channels\n",
    "                    dilation=conv_dilation,#卷积核元素之间的间距\n",
    "                    padding=\"same\",\n",
    "                )\n",
    "                for conv_dilation in range(1, conv_num + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print('input feature shape: {}'.format(x.shape))\n",
    "        capsules = [conv(x) for conv in self.primary_capsule_layer]\n",
    "        # capsules_reshaped = [\n",
    "        #    c.reshape(self.conv_num, self.feature_dimension) for c in capsules\n",
    "        # ]\n",
    "        output_tensor = torch.cat(capsules, dim=1)\n",
    "        return Squash()(output_tensor)\n",
    "\n",
    "\n",
    "def test_for_primary_capsule_layer():\n",
    "    input = torch.rand(1, 2, 105)\n",
    "    layer = PrimaryCapsuleLayer()\n",
    "    print(layer(input).shape)\n",
    "\n",
    "\n",
    "test_for_primary_capsule_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_capsules=10, in_caps=10, in_channels=272, out_channels=2#in_channels=105\n",
    "    ):\n",
    "        super(CapsLayer, self).__init__()\n",
    "        self.W = nn.Parameter(\n",
    "            torch.randn(1, num_capsules, in_caps, out_channels, in_channels)\n",
    "        )\n",
    "        # print('W shape: {}'.format(self.W.shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('CapsLayer input shape: {}'.format(x.shape))\n",
    "        x = x[:, None, ..., None]  # x.unsqueeze(1).unsqueeze(4)\n",
    "        # x = x.unsqueeze(-1)\n",
    "        #print('W shape: {}'.format(self.W.shape))\n",
    "        #print('CapsLayer input shape: {}'.format(x.shape))\n",
    "        # print('CapsLayer input expand shape: {}'.format(x[ :, :, None, :].shape))\n",
    "        # (batch_size, num_caps, num_route_nodes, out_channels, 1)\n",
    "        # print('x shape: {}'.format(x.shape))\n",
    "        u_hat = torch.matmul(self.W, x)  # (x @ self.W).squeeze(2)\n",
    "        # u=u_hat.squeeze(-1)\n",
    "        u_hat = u_hat.squeeze(-1)\n",
    "        #batch_size, num_caps, in_caps, out_channels\n",
    "        #print('u_hat\\'s batch_size: {}, num_caps: {}, in_caps: {}, out_channels: {}'.format(u_hat.shape[0], u_hat.shape[1], u_hat.shape[2], u_hat.shape[3]))\n",
    "        class_capsules = dynamic_routing(u_hat)\n",
    "        return class_capsules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1, 10, 10, 20, 1)\n",
    "b = torch.rand(1, 1, 10, 1, 1)\n",
    "c = a.matmul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CapsLayer input shape: torch.Size([2, 5, 105])\n",
      "W shape: torch.Size([1, 1, 5, 2, 105])\n",
      "CapsLayer input shape: torch.Size([2, 1, 5, 105, 1])\n",
      "u_hat's batch_size: 2, num_caps: 1, in_caps: 5, out_channels: 2\n",
      "input x's batch_size: 2, num_caps: 1, in_caps: 5, out_channels: 2\n",
      "softmax result's batch_size: 2, num_caps: 1, in_caps: 5, softmax_result: 1\n",
      "a's batch_size: 2, num_caps: 1, in_caps: 5, out_channels: 2\n",
      "s's batch_size: 2, num_caps: 1, out_channels: 2\n",
      "v's batch_size: 2, num_caps: 1, out_channels: 2\n",
      "x shape: torch.Size([2, 1, 5, 2])\n",
      "y shape: torch.Size([2, 1, 5, 1])\n",
      "b shape: torch.Size([2, 1, 5, 1])\n",
      "input x's batch_size: 2, num_caps: 1, in_caps: 5, out_channels: 2\n",
      "softmax result's batch_size: 2, num_caps: 1, in_caps: 5, softmax_result: 1\n",
      "a's batch_size: 2, num_caps: 1, in_caps: 5, out_channels: 2\n",
      "s's batch_size: 2, num_caps: 1, out_channels: 2\n",
      "v's batch_size: 2, num_caps: 1, out_channels: 2\n",
      "x shape: torch.Size([2, 1, 5, 2])\n",
      "y shape: torch.Size([2, 1, 5, 1])\n",
      "b shape: torch.Size([2, 1, 5, 1])\n",
      "input x's batch_size: 2, num_caps: 1, in_caps: 5, out_channels: 2\n",
      "softmax result's batch_size: 2, num_caps: 1, in_caps: 5, softmax_result: 1\n",
      "a's batch_size: 2, num_caps: 1, in_caps: 5, out_channels: 2\n",
      "s's batch_size: 2, num_caps: 1, out_channels: 2\n",
      "v's batch_size: 2, num_caps: 1, out_channels: 2\n",
      "x shape: torch.Size([2, 1, 5, 2])\n",
      "y shape: torch.Size([2, 1, 5, 1])\n",
      "b shape: torch.Size([2, 1, 5, 1])\n",
      "torch.Size([2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(2, 5, 105)\n",
    "layer = CapsLayer()\n",
    "print(layer(input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(512,1,272)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cpas shape: torch.Size([100, 34, 256])\n",
      "after primary caps shape: torch.Size([100, 20, 256])\n",
      "after caps layer shape: torch.Size([100, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 primary_conv=5,\n",
    "                 hidden_size=256,\n",
    "                 primary_conv_out=4, \n",
    "                 cap_output_num=16, \n",
    "                 vocab_size=16,\n",
    "                 embedding_size=16, \n",
    "                 dropout_rate=0.5, \n",
    "                 num_layers=3,\n",
    "                 hlstm_size=128):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.hlstm_size=hlstm_size\n",
    "        self.num_layers=num_layers\n",
    "        self.sig_len = 16\n",
    "        self.lstm_seq = nn.LSTM(self.sig_len, self.hlstm_size, self.num_layers,\n",
    "                            dropout=dropout_rate, batch_first=True, bidirectional=True)\n",
    "        self.lstm_sig = nn.LSTM(self.sig_len, self.hlstm_size, self.num_layers,\n",
    "                            dropout=dropout_rate, batch_first=True, bidirectional=True)\n",
    "        self.primary_layer = PrimaryCapsuleLayer(conv_in=17*2,conv_out=primary_conv_out,conv_num=primary_conv)\n",
    "        self.caps_layer = CapsLayer(num_capsules=10,in_caps=primary_conv_out*primary_conv,in_channels=2*hlstm_size,out_channels=cap_output_num)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(cap_output_num, hidden_size)  #\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def init_hidden(self, batch_size, num_layers, hidden_size):\n",
    "        # Set initial states\n",
    "        h0 = autograd.Variable(torch.randn(num_layers * 2, batch_size, hidden_size)).to(torch.float32)\n",
    "        c0 = autograd.Variable(torch.randn(num_layers * 2, batch_size, hidden_size)).to(torch.float32)\n",
    "        if use_cuda:\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "        return h0, c0\n",
    "\n",
    "    def forward(self, seq, sig):\n",
    "        seq_emb = self.embed(seq.long())#bacth_size, 17, 16\n",
    "        #seq_emb = seq_emb.unsqueeze(-1)#seq_emb.reshape(seq_emb.shape[0], 1, -1)\n",
    "        #sig = sig.unsqueeze(-1)#sig.reshape(sig.shape[0], 1, -1)\n",
    "        #print('seq_emb shape: {}'.format(seq_emb.shape))\n",
    "        #print('sig shape: {}'.format(sig.shape))\n",
    "        seq_emb,_=self.lstm_seq(seq_emb.to(torch.float32),self.init_hidden(seq_emb.size(0), self.num_layers,self.hlstm_size))\n",
    "        sig,_=self.lstm_seq(sig.to(torch.float32),self.init_hidden(sig.size(0), self.num_layers,self.hlstm_size))\n",
    "        #batch_size,sig_len,2*self.hlstm_size\n",
    "        #print('seq_emb shape: {}'.format(seq_emb.shape))\n",
    "        #print('sig shape: {}'.format(sig.shape))\n",
    "        # to(torch.float32) solve RuntimeError:expected scalar type Double but found Float\n",
    "        x = torch.cat((seq_emb, sig), dim=1).to(torch.float32)\n",
    "        #bach_size,kmer_len*2,self.hlstm_size*num_directions\n",
    "\n",
    "        #x,_ = self.lstm_comb(x,self.init_hidden(x.size(0), self.num_layers,self.hlstm_size))\n",
    "        \n",
    "        # seq = self.primary_layer(seq)\n",
    "        # seq = self.caps_layer(seq)\n",
    "        # sig = self.primary_layer(sig)\n",
    "        # sig = self.caps_layer(sig)\n",
    "        print('before cpas shape: {}'.format(x.shape))\n",
    "        x = self.primary_layer(x)\n",
    "        print('after primary caps shape: {}'.format(x.shape))\n",
    "        x = self.caps_layer(x)\n",
    "        print('after caps layer shape: {}'.format(x.shape))\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        # x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.relu2(x)\n",
    "        # print(x.shape)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        return x, self.softmax(x)\n",
    "\n",
    "\n",
    "\n",
    "def test_for_caps_net():\n",
    "    input1 = torch.rand(100, 17)\n",
    "    input2 = torch.rand(100, 17, 16)\n",
    "    \n",
    "    model = CapsNet()\n",
    "    model(input1,input2)\n",
    "\n",
    "\n",
    "test_for_caps_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight torch.Size([16, 16])\n",
      "lstm_seq.weight_ih_l0 torch.Size([512, 16])\n",
      "lstm_seq.weight_hh_l0 torch.Size([512, 128])\n",
      "lstm_seq.bias_ih_l0 torch.Size([512])\n",
      "lstm_seq.bias_hh_l0 torch.Size([512])\n",
      "lstm_seq.weight_ih_l0_reverse torch.Size([512, 16])\n",
      "lstm_seq.weight_hh_l0_reverse torch.Size([512, 128])\n",
      "lstm_seq.bias_ih_l0_reverse torch.Size([512])\n",
      "lstm_seq.bias_hh_l0_reverse torch.Size([512])\n",
      "lstm_seq.weight_ih_l1 torch.Size([512, 256])\n",
      "lstm_seq.weight_hh_l1 torch.Size([512, 128])\n",
      "lstm_seq.bias_ih_l1 torch.Size([512])\n",
      "lstm_seq.bias_hh_l1 torch.Size([512])\n",
      "lstm_seq.weight_ih_l1_reverse torch.Size([512, 256])\n",
      "lstm_seq.weight_hh_l1_reverse torch.Size([512, 128])\n",
      "lstm_seq.bias_ih_l1_reverse torch.Size([512])\n",
      "lstm_seq.bias_hh_l1_reverse torch.Size([512])\n",
      "lstm_seq.weight_ih_l2 torch.Size([512, 256])\n",
      "lstm_seq.weight_hh_l2 torch.Size([512, 128])\n",
      "lstm_seq.bias_ih_l2 torch.Size([512])\n",
      "lstm_seq.bias_hh_l2 torch.Size([512])\n",
      "lstm_seq.weight_ih_l2_reverse torch.Size([512, 256])\n",
      "lstm_seq.weight_hh_l2_reverse torch.Size([512, 128])\n",
      "lstm_seq.bias_ih_l2_reverse torch.Size([512])\n",
      "lstm_seq.bias_hh_l2_reverse torch.Size([512])\n",
      "lstm_sig.weight_ih_l0 torch.Size([512, 16])\n",
      "lstm_sig.weight_hh_l0 torch.Size([512, 128])\n",
      "lstm_sig.bias_ih_l0 torch.Size([512])\n",
      "lstm_sig.bias_hh_l0 torch.Size([512])\n",
      "lstm_sig.weight_ih_l0_reverse torch.Size([512, 16])\n",
      "lstm_sig.weight_hh_l0_reverse torch.Size([512, 128])\n",
      "lstm_sig.bias_ih_l0_reverse torch.Size([512])\n",
      "lstm_sig.bias_hh_l0_reverse torch.Size([512])\n",
      "lstm_sig.weight_ih_l1 torch.Size([512, 256])\n",
      "lstm_sig.weight_hh_l1 torch.Size([512, 128])\n",
      "lstm_sig.bias_ih_l1 torch.Size([512])\n",
      "lstm_sig.bias_hh_l1 torch.Size([512])\n",
      "lstm_sig.weight_ih_l1_reverse torch.Size([512, 256])\n",
      "lstm_sig.weight_hh_l1_reverse torch.Size([512, 128])\n",
      "lstm_sig.bias_ih_l1_reverse torch.Size([512])\n",
      "lstm_sig.bias_hh_l1_reverse torch.Size([512])\n",
      "lstm_sig.weight_ih_l2 torch.Size([512, 256])\n",
      "lstm_sig.weight_hh_l2 torch.Size([512, 128])\n",
      "lstm_sig.bias_ih_l2 torch.Size([512])\n",
      "lstm_sig.bias_hh_l2 torch.Size([512])\n",
      "lstm_sig.weight_ih_l2_reverse torch.Size([512, 256])\n",
      "lstm_sig.weight_hh_l2_reverse torch.Size([512, 128])\n",
      "lstm_sig.bias_ih_l2_reverse torch.Size([512])\n",
      "lstm_sig.bias_hh_l2_reverse torch.Size([512])\n",
      "primary_layer.primary_capsule_layer.0.weight torch.Size([4, 34, 2])\n",
      "primary_layer.primary_capsule_layer.0.bias torch.Size([4])\n",
      "primary_layer.primary_capsule_layer.1.weight torch.Size([4, 34, 2])\n",
      "primary_layer.primary_capsule_layer.1.bias torch.Size([4])\n",
      "primary_layer.primary_capsule_layer.2.weight torch.Size([4, 34, 2])\n",
      "primary_layer.primary_capsule_layer.2.bias torch.Size([4])\n",
      "primary_layer.primary_capsule_layer.3.weight torch.Size([4, 34, 2])\n",
      "primary_layer.primary_capsule_layer.3.bias torch.Size([4])\n",
      "primary_layer.primary_capsule_layer.4.weight torch.Size([4, 34, 2])\n",
      "primary_layer.primary_capsule_layer.4.bias torch.Size([4])\n",
      "caps_layer.W torch.Size([1, 10, 20, 16, 256])\n",
      "fc1.weight torch.Size([256, 16])\n",
      "fc1.bias torch.Size([256])\n",
      "fc2.weight torch.Size([2, 256])\n",
      "fc2.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "model =CapsNet()\n",
    "freeze_layers_prefix = (\"embed\", \"lstm_seq\",\"lstm_sig\",\"primary_layer\",\"caps_layer\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    if name.split('.')[0] in freeze_layers_prefix:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, classes, labels):\n",
    "        #classes = classes.reshape(classes.shape[0], 2)\n",
    "        #labels = labels.reshape(labels.shape[0], 1)\n",
    "        #print('classes shape: {}'.format(classes.shape))\n",
    "        #print('labels shape: {}'.format(labels.shape))\n",
    "        #left = F.relu(0.9 - classes[0], inplace=True) ** 2\n",
    "        #print('left shape: {}'.format(left.shape))\n",
    "        #right = F.relu(classes[1] - 0.1, inplace=True) ** 2\n",
    "        #print('right shape: {}'.format(right.shape))\n",
    "\n",
    "        #margin_loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "        #margin_loss = margin_loss.sum()\n",
    "        return self.loss(classes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import linecache\n",
    "\n",
    "base2code_dna = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3, \"N\": 4}\n",
    "code2base_dna = {0: \"A\", 1: \"C\", 2: \"G\", 3: \"T\", 4: \"N\"}\n",
    "\n",
    "\n",
    "def clear_linecache():\n",
    "    # linecache should be treated carefully\n",
    "    linecache.clearcache()\n",
    "\n",
    "\n",
    "def parse_a_line(line):\n",
    "    words = line.strip().split(\"\\t\")\n",
    "\n",
    "    seq = np.array(\n",
    "        [[base2code_dna[y] for y in x.split(\",\")] for x in words[1].split(\";\")]\n",
    "    )\n",
    "    signal = np.array(\n",
    "        [[np.float16(y) for y in x.split(\",\")] for x in words[2].split(\";\")]\n",
    "    )\n",
    "    label = np.random.randint(0, 2)\n",
    "    #if rlabel==1:\n",
    "    #    label=[0,1]\n",
    "    #else:\n",
    "    #    label=[1,0]\n",
    "\n",
    "    return seq, signal, label\n",
    "\n",
    "\n",
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, filename, transform=None):\n",
    "        # print(\">>>using linecache to access '{}'<<<\\n\"\n",
    "        #       \">>>after done using the file, \"\n",
    "        #       \"remember to use linecache.clearcache() to clear cache for safety<<<\".format(filename))\n",
    "        self._filename = os.path.abspath(filename)\n",
    "        self._total_data = 0\n",
    "        self._transform = transform\n",
    "        with open(filename, \"r\") as f:\n",
    "            self._total_data = len(f.readlines())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = linecache.getline(self._filename, idx + 1)\n",
    "        if line == \"\":\n",
    "            return None\n",
    "        else:\n",
    "            output = parse_a_line(line)\n",
    "            if self._transform is not None:\n",
    "                output = self._transform(output)\n",
    "            return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._total_data\n",
    "\n",
    "def parse_a_line2(line):\n",
    "    words = line.strip().split(\"\\t\")\n",
    "\n",
    "    sampleinfo = \"\\t\".join(words[0:6])\n",
    "\n",
    "    kmer = np.array([base2code_dna[x] for x in words[6]])\n",
    "    base_means = np.array([float(x) for x in words[7].split(\",\")])\n",
    "    base_stds = np.array([float(x) for x in words[8].split(\",\")])\n",
    "    base_signal_lens = np.array([int(x) for x in words[9].split(\",\")])\n",
    "    k_signals = np.array([[float(y) for y in x.split(\",\")] for x in words[10].split(\";\")])\n",
    "    label = int(words[11])\n",
    "\n",
    "    return kmer, k_signals, label   \n",
    "class SignalFeaData2(Dataset):\n",
    "    def __init__(self, filename, transform=None):\n",
    "        # print(\">>>using linecache to access '{}'<<<\\n\"\n",
    "        #       \">>>after done using the file, \"\n",
    "        #       \"remember to use linecache.clearcache() to clear cache for safety<<<\".format(filename))\n",
    "        self._filename = os.path.abspath(filename)\n",
    "        self._total_data = 0\n",
    "        self._transform = transform\n",
    "        with open(filename, \"r\") as f:\n",
    "            self._total_data = len(f.readlines())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = linecache.getline(self._filename, idx + 1)\n",
    "        if line == \"\":\n",
    "            return None\n",
    "        else:\n",
    "            output = parse_a_line2(line)\n",
    "            if self._transform is not None:\n",
    "                output = self._transform(output)\n",
    "            return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._total_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from torch.utils.data import sampler\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_start = time.time()\n",
    "    args = parse_args()\n",
    "    train_dataset = SignalFeaData2(args.train_file)\n",
    "\n",
    "    split_num = int(len(train_dataset) * 0.8)\n",
    "    index_list = list(range(len(train_dataset)))\n",
    "    train_idx, val_idx = index_list[:split_num], index_list[split_num:]\n",
    " \n",
    "    train_sampler = sampler.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = sampler.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset, batch_size=args.batch_size, sampler=train_sampler\n",
    "    )\n",
    "    total_step = len(train_loader)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset, batch_size=args.batch_size, sampler=val_sampler\n",
    "    )\n",
    "    model = CapsNet()\n",
    "    criterion = CapsuleLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "    curr_best_accuracy = 0\n",
    "    model_dir = args.model_dir\n",
    "    if model_dir != \"/\":\n",
    "        model_dir = os.path.abspath(model_dir).rstrip(\"/\")\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        else:\n",
    "            model_regex = re.compile(\n",
    "                r\"\" + \"\\.b\\d+_s\\d+_epoch\\d+\\.ckpt*\"\n",
    "            )\n",
    "            for mfile in os.listdir(model_dir):\n",
    "                if model_regex.match(mfile):\n",
    "                    os.remove(model_dir + \"/\" + mfile)\n",
    "        model_dir += \"/\"\n",
    "    model.train()\n",
    "    for epoch in range(args.max_epoch_num):\n",
    "        curr_best_accuracy_epoch = 0\n",
    "        no_best_model = True\n",
    "        tlosses = []\n",
    "        start = time.time()\n",
    "        for i, sfeatures in tqdm(enumerate(train_loader)):\n",
    "            (seq, signal, labels) = sfeatures\n",
    "            outputs = model(seq, signal)\n",
    "            loss = criterion(outputs, labels)\n",
    "            #print(loss)\n",
    "            print(loss.detach().item())\n",
    "            tlosses.append(loss.detach().item())\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            if (i + 1) % args.step_interval == 0 or i == total_step - 1:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    vlosses, vlabels_total, vpredicted_total = [], [], []\n",
    "                    for vi, vsfeatures in tqdm(enumerate(valid_loader)):\n",
    "                        (\n",
    "                            vseq,\n",
    "                            vsignal,\n",
    "                            vlabels,\n",
    "                        ) = vsfeatures\n",
    "                        voutputs = model(vseq, vsignal)\n",
    "                        vloss = criterion(voutputs, vlabels)\n",
    "\n",
    "                        _, vpredicted = torch.max(voutputs.data, 1)\n",
    "                        #print(vpredicted)\n",
    "                        vlosses.append(vloss.item())\n",
    "                        vlabels_total += vlabels.tolist()\n",
    "                        vpredicted_total += vpredicted.tolist()\n",
    "                        v_accuracy = metrics.accuracy_score(\n",
    "                            vlabels_total, vpredicted_total\n",
    "                        )\n",
    "                        v_precision = metrics.precision_score(\n",
    "                            vlabels_total, vpredicted_total\n",
    "                        )\n",
    "                        v_recall = metrics.recall_score(vlabels_total, vpredicted_total)\n",
    "                        if v_accuracy > curr_best_accuracy_epoch:\n",
    "                            curr_best_accuracy_epoch = v_accuracy\n",
    "                            if curr_best_accuracy_epoch > curr_best_accuracy - 0.0002:\n",
    "                                torch.save(\n",
    "                                    model.state_dict(),\n",
    "                                    model_dir\n",
    "                                    + \".epoch{}.ckpt\".format(\n",
    "                                        epoch\n",
    "                                    ),\n",
    "                                )\n",
    "                                if curr_best_accuracy_epoch > curr_best_accuracy:\n",
    "                                    curr_best_accuracy = curr_best_accuracy_epoch\n",
    "                                    no_best_model = False\n",
    "                        time_cost = time.time() - start\n",
    "                        print(\n",
    "                            \"Epoch [{}/{}], Step [{}/{}], TrainLoss: {:.4f}; \"\n",
    "                            \"ValidLoss: {:.4f}, \"\n",
    "                            \"Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, \"\n",
    "                            \"curr_epoch_best_accuracy: {:.4f}; Time: {:.2f}s\".format(\n",
    "                                epoch + 1,\n",
    "                                args.max_epoch_num,\n",
    "                                i + 1,\n",
    "                                total_step,\n",
    "                                np.mean(tlosses),\n",
    "                                np.mean(vlosses),\n",
    "                                v_accuracy,\n",
    "                                v_precision,\n",
    "                                v_recall,\n",
    "                                curr_best_accuracy_epoch,\n",
    "                                time_cost,\n",
    "                            )\n",
    "                        )\n",
    "                        tlosses = []\n",
    "                        start = time.time()\n",
    "                        sys.stdout.flush()\n",
    "                    model.train()\n",
    "            scheduler.step()\n",
    "            if no_best_model and epoch >= args.min_epoch_num - 1:\n",
    "                print(\"early stop!\")\n",
    "                break\n",
    "        endtime = time.time()\n",
    "        clear_linecache()\n",
    "        print(\n",
    "            \"[main] train costs {} seconds, \"\n",
    "            \"best accuracy: {}\".format(endtime - total_start, curr_best_accuracy)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsignal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
